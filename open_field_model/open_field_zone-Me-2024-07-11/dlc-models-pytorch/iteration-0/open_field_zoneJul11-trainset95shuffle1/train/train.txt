2024-07-22 18:07:06 Training with configuration:
2024-07-22 18:07:06 data:
2024-07-22 18:07:06   colormode: RGB
2024-07-22 18:07:06   inference:
2024-07-22 18:07:06     normalize_images: True
2024-07-22 18:07:06     auto_padding:
2024-07-22 18:07:06       pad_width_divisor: 32
2024-07-22 18:07:06       pad_height_divisor: 32
2024-07-22 18:07:06   train:
2024-07-22 18:07:06     affine:
2024-07-22 18:07:06       p: 0.5
2024-07-22 18:07:06       rotation: 30
2024-07-22 18:07:06       scaling: [1.0, 1.0]
2024-07-22 18:07:06       translation: 0
2024-07-22 18:07:06     collate:
2024-07-22 18:07:06       type: ResizeFromDataSizeCollate
2024-07-22 18:07:06       min_scale: 0.4
2024-07-22 18:07:06       max_scale: 1.0
2024-07-22 18:07:06       min_short_side: 128
2024-07-22 18:07:06       max_short_side: 1152
2024-07-22 18:07:06       multiple_of: 32
2024-07-22 18:07:06       to_square: False
2024-07-22 18:07:06     covering: False
2024-07-22 18:07:06     gaussian_noise: 12.75
2024-07-22 18:07:06     hist_eq: False
2024-07-22 18:07:06     motion_blur: False
2024-07-22 18:07:06     normalize_images: True
2024-07-22 18:07:06 device: auto
2024-07-22 18:07:06 metadata:
2024-07-22 18:07:06   project_path: C:\Users\schiv\OneDrive\Desktop\Cours\Master\GSON\Stage_M1\Travail\Scripts\open_field_model\open_field_zone-Me-2024-07-11
2024-07-22 18:07:06   pose_config_path: C:\Users\schiv\OneDrive\Desktop\Cours\Master\GSON\Stage_M1\Travail\Scripts\open_field_model\open_field_zone-Me-2024-07-11\dlc-models-pytorch\iteration-0\open_field_zoneJul11-trainset95shuffle1\train\pose_cfg.yaml
2024-07-22 18:07:06   bodyparts: ['Tete', 'Corps', 'Queue']
2024-07-22 18:07:06   unique_bodyparts: []
2024-07-22 18:07:06   individuals: ['animal']
2024-07-22 18:07:06   with_identity: None
2024-07-22 18:07:06 method: bu
2024-07-22 18:07:06 model:
2024-07-22 18:07:06   backbone:
2024-07-22 18:07:06     type: HRNet
2024-07-22 18:07:06     model_name: hrnet_w48
2024-07-22 18:07:06     freeze_bn_stats: True
2024-07-22 18:07:06     freeze_bn_weights: False
2024-07-22 18:07:06     interpolate_branches: False
2024-07-22 18:07:06     increased_channel_count: False
2024-07-22 18:07:06   backbone_output_channels: 48
2024-07-22 18:07:06   heads:
2024-07-22 18:07:06     bodypart:
2024-07-22 18:07:06       type: HeatmapHead
2024-07-22 18:07:06       weight_init: normal
2024-07-22 18:07:06       predictor:
2024-07-22 18:07:06         type: HeatmapPredictor
2024-07-22 18:07:06         apply_sigmoid: False
2024-07-22 18:07:06         clip_scores: True
2024-07-22 18:07:06         location_refinement: True
2024-07-22 18:07:06         locref_std: 7.2801
2024-07-22 18:07:06       target_generator:
2024-07-22 18:07:06         type: HeatmapGaussianGenerator
2024-07-22 18:07:06         num_heatmaps: 3
2024-07-22 18:07:06         pos_dist_thresh: 17
2024-07-22 18:07:06         heatmap_mode: KEYPOINT
2024-07-22 18:07:06         generate_locref: True
2024-07-22 18:07:06         locref_std: 7.2801
2024-07-22 18:07:06       criterion:
2024-07-22 18:07:06         heatmap:
2024-07-22 18:07:06           type: WeightedMSECriterion
2024-07-22 18:07:06           weight: 1.0
2024-07-22 18:07:06         locref:
2024-07-22 18:07:06           type: WeightedHuberCriterion
2024-07-22 18:07:06           weight: 0.05
2024-07-22 18:07:06       heatmap_config:
2024-07-22 18:07:06         channels: [48, 3]
2024-07-22 18:07:06         kernel_size: [3]
2024-07-22 18:07:06         strides: [2]
2024-07-22 18:07:06       locref_config:
2024-07-22 18:07:06         channels: [48, 6]
2024-07-22 18:07:06         kernel_size: [3]
2024-07-22 18:07:06         strides: [2]
2024-07-22 18:07:06 net_type: hrnet_w48
2024-07-22 18:07:06 runner:
2024-07-22 18:07:06   type: PoseTrainingRunner
2024-07-22 18:07:06   gpus: None
2024-07-22 18:07:06   key_metric: test.mAP
2024-07-22 18:07:06   key_metric_asc: True
2024-07-22 18:07:06   eval_interval: 1
2024-07-22 18:07:06   optimizer:
2024-07-22 18:07:06     type: AdamW
2024-07-22 18:07:06     params:
2024-07-22 18:07:06       lr: 0.0001
2024-07-22 18:07:06   scheduler:
2024-07-22 18:07:06     type: LRListScheduler
2024-07-22 18:07:06     params:
2024-07-22 18:07:06       lr_list: [[1e-05], [1e-06]]
2024-07-22 18:07:06       milestones: [160, 190]
2024-07-22 18:07:06   snapshots:
2024-07-22 18:07:06     max_snapshots: 5
2024-07-22 18:07:06     save_epochs: 25
2024-07-22 18:07:06     save_optimizer_state: False
2024-07-22 18:07:06 train_settings:
2024-07-22 18:07:06   batch_size: 1
2024-07-22 18:07:06   dataloader_workers: 0
2024-07-22 18:07:06   dataloader_pin_memory: True
2024-07-22 18:07:06   display_iters: 500
2024-07-22 18:07:06   epochs: 200
2024-07-22 18:07:06   seed: 42
2024-07-22 18:07:07 Loading pretrained weights from Hugging Face hub (timm/hrnet_w48.ms_in1k)
2024-07-22 18:07:34 [timm/hrnet_w48.ms_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2024-07-22 18:07:34 Unexpected keys (downsamp_modules.0.0.bias, downsamp_modules.0.0.weight, downsamp_modules.0.1.bias, downsamp_modules.0.1.num_batches_tracked, downsamp_modules.0.1.running_mean, downsamp_modules.0.1.running_var, downsamp_modules.0.1.weight, downsamp_modules.1.0.bias, downsamp_modules.1.0.weight, downsamp_modules.1.1.bias, downsamp_modules.1.1.num_batches_tracked, downsamp_modules.1.1.running_mean, downsamp_modules.1.1.running_var, downsamp_modules.1.1.weight, downsamp_modules.2.0.bias, downsamp_modules.2.0.weight, downsamp_modules.2.1.bias, downsamp_modules.2.1.num_batches_tracked, downsamp_modules.2.1.running_mean, downsamp_modules.2.1.running_var, downsamp_modules.2.1.weight, final_layer.0.bias, final_layer.0.weight, final_layer.1.bias, final_layer.1.num_batches_tracked, final_layer.1.running_mean, final_layer.1.running_var, final_layer.1.weight, incre_modules.0.0.bn1.bias, incre_modules.0.0.bn1.num_batches_tracked, incre_modules.0.0.bn1.running_mean, incre_modules.0.0.bn1.running_var, incre_modules.0.0.bn1.weight, incre_modules.0.0.bn2.bias, incre_modules.0.0.bn2.num_batches_tracked, incre_modules.0.0.bn2.running_mean, incre_modules.0.0.bn2.running_var, incre_modules.0.0.bn2.weight, incre_modules.0.0.bn3.bias, incre_modules.0.0.bn3.num_batches_tracked, incre_modules.0.0.bn3.running_mean, incre_modules.0.0.bn3.running_var, incre_modules.0.0.bn3.weight, incre_modules.0.0.conv1.weight, incre_modules.0.0.conv2.weight, incre_modules.0.0.conv3.weight, incre_modules.0.0.downsample.0.weight, incre_modules.0.0.downsample.1.bias, incre_modules.0.0.downsample.1.num_batches_tracked, incre_modules.0.0.downsample.1.running_mean, incre_modules.0.0.downsample.1.running_var, incre_modules.0.0.downsample.1.weight, incre_modules.1.0.bn1.bias, incre_modules.1.0.bn1.num_batches_tracked, incre_modules.1.0.bn1.running_mean, incre_modules.1.0.bn1.running_var, incre_modules.1.0.bn1.weight, incre_modules.1.0.bn2.bias, incre_modules.1.0.bn2.num_batches_tracked, incre_modules.1.0.bn2.running_mean, incre_modules.1.0.bn2.running_var, incre_modules.1.0.bn2.weight, incre_modules.1.0.bn3.bias, incre_modules.1.0.bn3.num_batches_tracked, incre_modules.1.0.bn3.running_mean, incre_modules.1.0.bn3.running_var, incre_modules.1.0.bn3.weight, incre_modules.1.0.conv1.weight, incre_modules.1.0.conv2.weight, incre_modules.1.0.conv3.weight, incre_modules.1.0.downsample.0.weight, incre_modules.1.0.downsample.1.bias, incre_modules.1.0.downsample.1.num_batches_tracked, incre_modules.1.0.downsample.1.running_mean, incre_modules.1.0.downsample.1.running_var, incre_modules.1.0.downsample.1.weight, incre_modules.2.0.bn1.bias, incre_modules.2.0.bn1.num_batches_tracked, incre_modules.2.0.bn1.running_mean, incre_modules.2.0.bn1.running_var, incre_modules.2.0.bn1.weight, incre_modules.2.0.bn2.bias, incre_modules.2.0.bn2.num_batches_tracked, incre_modules.2.0.bn2.running_mean, incre_modules.2.0.bn2.running_var, incre_modules.2.0.bn2.weight, incre_modules.2.0.bn3.bias, incre_modules.2.0.bn3.num_batches_tracked, incre_modules.2.0.bn3.running_mean, incre_modules.2.0.bn3.running_var, incre_modules.2.0.bn3.weight, incre_modules.2.0.conv1.weight, incre_modules.2.0.conv2.weight, incre_modules.2.0.conv3.weight, incre_modules.2.0.downsample.0.weight, incre_modules.2.0.downsample.1.bias, incre_modules.2.0.downsample.1.num_batches_tracked, incre_modules.2.0.downsample.1.running_mean, incre_modules.2.0.downsample.1.running_var, incre_modules.2.0.downsample.1.weight, incre_modules.3.0.bn1.bias, incre_modules.3.0.bn1.num_batches_tracked, incre_modules.3.0.bn1.running_mean, incre_modules.3.0.bn1.running_var, incre_modules.3.0.bn1.weight, incre_modules.3.0.bn2.bias, incre_modules.3.0.bn2.num_batches_tracked, incre_modules.3.0.bn2.running_mean, incre_modules.3.0.bn2.running_var, incre_modules.3.0.bn2.weight, incre_modules.3.0.bn3.bias, incre_modules.3.0.bn3.num_batches_tracked, incre_modules.3.0.bn3.running_mean, incre_modules.3.0.bn3.running_var, incre_modules.3.0.bn3.weight, incre_modules.3.0.conv1.weight, incre_modules.3.0.conv2.weight, incre_modules.3.0.conv3.weight, incre_modules.3.0.downsample.0.weight, incre_modules.3.0.downsample.1.bias, incre_modules.3.0.downsample.1.num_batches_tracked, incre_modules.3.0.downsample.1.running_mean, incre_modules.3.0.downsample.1.running_var, incre_modules.3.0.downsample.1.weight, classifier.bias, classifier.weight) found while loading pretrained weights. This may be expected if model is being adapted.
2024-07-22 18:07:34 Data Transforms:
2024-07-22 18:07:34   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2024-07-22 18:07:34   Validation: Compose([
  PadIfNeeded(always_apply=False, p=1.0, min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, position=PositionType.RANDOM, border_mode=4, value=None, mask_value=None),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2024-07-22 18:07:34 Using custom collate function: {'type': 'ResizeFromDataSizeCollate', 'min_scale': 0.4, 'max_scale': 1.0, 'min_short_side': 128, 'max_short_side': 1152, 'multiple_of': 32, 'to_square': False}
2024-07-22 18:07:34 
Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.
This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.
If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. 
This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).

2024-07-22 18:07:34 Using 114 images and 6 for testing
2024-07-22 18:07:34 
Starting pose model training...
--------------------------------------------------
2024-07-22 18:07:55 Training for epoch 1 done, starting evaluation
2024-07-22 18:07:55 Epoch 1 performance:
2024-07-22 18:07:55 metrics/test.rmse:  24.528
2024-07-22 18:07:55 metrics/test.rmse_pcutoff:18.649
2024-07-22 18:07:55 metrics/test.mAP:   0.561
2024-07-22 18:07:55 metrics/test.mAR:   1.667
2024-07-22 18:07:55 metrics/test.mAP_pcutoff:0.000
2024-07-22 18:07:55 metrics/test.mAR_pcutoff:0.000
2024-07-22 18:07:55 Epoch 1/200 (lr=0.0001), train loss 0.00998, valid loss 0.00654
2024-07-22 18:08:11 Training for epoch 2 done, starting evaluation
2024-07-22 18:08:11 Epoch 2 performance:
2024-07-22 18:08:11 metrics/test.rmse:  8.589
2024-07-22 18:08:11 metrics/test.rmse_pcutoff:4.370
2024-07-22 18:08:11 metrics/test.mAP:   22.525
2024-07-22 18:08:11 metrics/test.mAR:   30.000
2024-07-22 18:08:11 metrics/test.mAP_pcutoff:12.970
2024-07-22 18:08:11 metrics/test.mAR_pcutoff:16.667
2024-07-22 18:08:11 Epoch 2/200 (lr=0.0001), train loss 0.00512, valid loss 0.00539
2024-07-23 13:17:06 Training with configuration:
2024-07-23 13:17:06 data:
2024-07-23 13:17:06   colormode: RGB
2024-07-23 13:17:06   inference:
2024-07-23 13:17:06     normalize_images: True
2024-07-23 13:17:06     auto_padding:
2024-07-23 13:17:06       pad_width_divisor: 32
2024-07-23 13:17:06       pad_height_divisor: 32
2024-07-23 13:17:06   train:
2024-07-23 13:17:06     affine:
2024-07-23 13:17:06       p: 0.5
2024-07-23 13:17:06       rotation: 30
2024-07-23 13:17:06       scaling: [1.0, 1.0]
2024-07-23 13:17:06       translation: 0
2024-07-23 13:17:06     collate:
2024-07-23 13:17:06       type: ResizeFromDataSizeCollate
2024-07-23 13:17:06       min_scale: 0.4
2024-07-23 13:17:06       max_scale: 1.0
2024-07-23 13:17:06       min_short_side: 128
2024-07-23 13:17:06       max_short_side: 1152
2024-07-23 13:17:06       multiple_of: 32
2024-07-23 13:17:06       to_square: False
2024-07-23 13:17:06     covering: False
2024-07-23 13:17:06     gaussian_noise: 12.75
2024-07-23 13:17:06     hist_eq: False
2024-07-23 13:17:06     motion_blur: False
2024-07-23 13:17:06     normalize_images: True
2024-07-23 13:17:06 device: auto
2024-07-23 13:17:06 metadata:
2024-07-23 13:17:06   project_path: C:\Users\schiv\OneDrive\Desktop\Cours\Master\GSON\Stage_M1\Travail\Scripts\open_field_model\open_field_zone-Me-2024-07-11
2024-07-23 13:17:06   pose_config_path: C:\Users\schiv\OneDrive\Desktop\Cours\Master\GSON\Stage_M1\Travail\Scripts\open_field_model\open_field_zone-Me-2024-07-11\dlc-models-pytorch\iteration-0\open_field_zoneJul11-trainset95shuffle1\train\pose_cfg.yaml
2024-07-23 13:17:06   bodyparts: ['Tete', 'Corps', 'Queue']
2024-07-23 13:17:06   unique_bodyparts: []
2024-07-23 13:17:06   individuals: ['animal']
2024-07-23 13:17:06   with_identity: None
2024-07-23 13:17:06 method: bu
2024-07-23 13:17:06 model:
2024-07-23 13:17:06   backbone:
2024-07-23 13:17:06     type: HRNet
2024-07-23 13:17:06     model_name: hrnet_w48
2024-07-23 13:17:06     freeze_bn_stats: True
2024-07-23 13:17:06     freeze_bn_weights: False
2024-07-23 13:17:06     interpolate_branches: False
2024-07-23 13:17:06     increased_channel_count: False
2024-07-23 13:17:06   backbone_output_channels: 48
2024-07-23 13:17:06   heads:
2024-07-23 13:17:06     bodypart:
2024-07-23 13:17:06       type: HeatmapHead
2024-07-23 13:17:06       weight_init: normal
2024-07-23 13:17:06       predictor:
2024-07-23 13:17:06         type: HeatmapPredictor
2024-07-23 13:17:06         apply_sigmoid: False
2024-07-23 13:17:06         clip_scores: True
2024-07-23 13:17:06         location_refinement: True
2024-07-23 13:17:06         locref_std: 7.2801
2024-07-23 13:17:06       target_generator:
2024-07-23 13:17:06         type: HeatmapGaussianGenerator
2024-07-23 13:17:06         num_heatmaps: 3
2024-07-23 13:17:06         pos_dist_thresh: 17
2024-07-23 13:17:06         heatmap_mode: KEYPOINT
2024-07-23 13:17:06         generate_locref: True
2024-07-23 13:17:06         locref_std: 7.2801
2024-07-23 13:17:06       criterion:
2024-07-23 13:17:06         heatmap:
2024-07-23 13:17:06           type: WeightedMSECriterion
2024-07-23 13:17:06           weight: 1.0
2024-07-23 13:17:06         locref:
2024-07-23 13:17:06           type: WeightedHuberCriterion
2024-07-23 13:17:06           weight: 0.05
2024-07-23 13:17:06       heatmap_config:
2024-07-23 13:17:06         channels: [48, 3]
2024-07-23 13:17:06         kernel_size: [3]
2024-07-23 13:17:06         strides: [2]
2024-07-23 13:17:06       locref_config:
2024-07-23 13:17:06         channels: [48, 6]
2024-07-23 13:17:06         kernel_size: [3]
2024-07-23 13:17:06         strides: [2]
2024-07-23 13:17:06 net_type: hrnet_w48
2024-07-23 13:17:06 runner:
2024-07-23 13:17:06   type: PoseTrainingRunner
2024-07-23 13:17:06   gpus: None
2024-07-23 13:17:06   key_metric: test.mAP
2024-07-23 13:17:06   key_metric_asc: True
2024-07-23 13:17:06   eval_interval: 1
2024-07-23 13:17:06   optimizer:
2024-07-23 13:17:06     type: AdamW
2024-07-23 13:17:06     params:
2024-07-23 13:17:06       lr: 0.0001
2024-07-23 13:17:06   scheduler:
2024-07-23 13:17:06     type: LRListScheduler
2024-07-23 13:17:06     params:
2024-07-23 13:17:06       lr_list: [[1e-05], [1e-06]]
2024-07-23 13:17:06       milestones: [160, 190]
2024-07-23 13:17:06   snapshots:
2024-07-23 13:17:06     max_snapshots: 5
2024-07-23 13:17:06     save_epochs: 25
2024-07-23 13:17:06     save_optimizer_state: False
2024-07-23 13:17:06 train_settings:
2024-07-23 13:17:06   batch_size: 1
2024-07-23 13:17:06   dataloader_workers: 0
2024-07-23 13:17:06   dataloader_pin_memory: True
2024-07-23 13:17:06   display_iters: 500
2024-07-23 13:17:06   epochs: 200
2024-07-23 13:17:06   seed: 42
2024-07-23 13:17:07 Loading pretrained weights from Hugging Face hub (timm/hrnet_w48.ms_in1k)
2024-07-23 13:17:07 [timm/hrnet_w48.ms_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2024-07-23 13:17:07 Unexpected keys (downsamp_modules.0.0.bias, downsamp_modules.0.0.weight, downsamp_modules.0.1.bias, downsamp_modules.0.1.num_batches_tracked, downsamp_modules.0.1.running_mean, downsamp_modules.0.1.running_var, downsamp_modules.0.1.weight, downsamp_modules.1.0.bias, downsamp_modules.1.0.weight, downsamp_modules.1.1.bias, downsamp_modules.1.1.num_batches_tracked, downsamp_modules.1.1.running_mean, downsamp_modules.1.1.running_var, downsamp_modules.1.1.weight, downsamp_modules.2.0.bias, downsamp_modules.2.0.weight, downsamp_modules.2.1.bias, downsamp_modules.2.1.num_batches_tracked, downsamp_modules.2.1.running_mean, downsamp_modules.2.1.running_var, downsamp_modules.2.1.weight, final_layer.0.bias, final_layer.0.weight, final_layer.1.bias, final_layer.1.num_batches_tracked, final_layer.1.running_mean, final_layer.1.running_var, final_layer.1.weight, incre_modules.0.0.bn1.bias, incre_modules.0.0.bn1.num_batches_tracked, incre_modules.0.0.bn1.running_mean, incre_modules.0.0.bn1.running_var, incre_modules.0.0.bn1.weight, incre_modules.0.0.bn2.bias, incre_modules.0.0.bn2.num_batches_tracked, incre_modules.0.0.bn2.running_mean, incre_modules.0.0.bn2.running_var, incre_modules.0.0.bn2.weight, incre_modules.0.0.bn3.bias, incre_modules.0.0.bn3.num_batches_tracked, incre_modules.0.0.bn3.running_mean, incre_modules.0.0.bn3.running_var, incre_modules.0.0.bn3.weight, incre_modules.0.0.conv1.weight, incre_modules.0.0.conv2.weight, incre_modules.0.0.conv3.weight, incre_modules.0.0.downsample.0.weight, incre_modules.0.0.downsample.1.bias, incre_modules.0.0.downsample.1.num_batches_tracked, incre_modules.0.0.downsample.1.running_mean, incre_modules.0.0.downsample.1.running_var, incre_modules.0.0.downsample.1.weight, incre_modules.1.0.bn1.bias, incre_modules.1.0.bn1.num_batches_tracked, incre_modules.1.0.bn1.running_mean, incre_modules.1.0.bn1.running_var, incre_modules.1.0.bn1.weight, incre_modules.1.0.bn2.bias, incre_modules.1.0.bn2.num_batches_tracked, incre_modules.1.0.bn2.running_mean, incre_modules.1.0.bn2.running_var, incre_modules.1.0.bn2.weight, incre_modules.1.0.bn3.bias, incre_modules.1.0.bn3.num_batches_tracked, incre_modules.1.0.bn3.running_mean, incre_modules.1.0.bn3.running_var, incre_modules.1.0.bn3.weight, incre_modules.1.0.conv1.weight, incre_modules.1.0.conv2.weight, incre_modules.1.0.conv3.weight, incre_modules.1.0.downsample.0.weight, incre_modules.1.0.downsample.1.bias, incre_modules.1.0.downsample.1.num_batches_tracked, incre_modules.1.0.downsample.1.running_mean, incre_modules.1.0.downsample.1.running_var, incre_modules.1.0.downsample.1.weight, incre_modules.2.0.bn1.bias, incre_modules.2.0.bn1.num_batches_tracked, incre_modules.2.0.bn1.running_mean, incre_modules.2.0.bn1.running_var, incre_modules.2.0.bn1.weight, incre_modules.2.0.bn2.bias, incre_modules.2.0.bn2.num_batches_tracked, incre_modules.2.0.bn2.running_mean, incre_modules.2.0.bn2.running_var, incre_modules.2.0.bn2.weight, incre_modules.2.0.bn3.bias, incre_modules.2.0.bn3.num_batches_tracked, incre_modules.2.0.bn3.running_mean, incre_modules.2.0.bn3.running_var, incre_modules.2.0.bn3.weight, incre_modules.2.0.conv1.weight, incre_modules.2.0.conv2.weight, incre_modules.2.0.conv3.weight, incre_modules.2.0.downsample.0.weight, incre_modules.2.0.downsample.1.bias, incre_modules.2.0.downsample.1.num_batches_tracked, incre_modules.2.0.downsample.1.running_mean, incre_modules.2.0.downsample.1.running_var, incre_modules.2.0.downsample.1.weight, incre_modules.3.0.bn1.bias, incre_modules.3.0.bn1.num_batches_tracked, incre_modules.3.0.bn1.running_mean, incre_modules.3.0.bn1.running_var, incre_modules.3.0.bn1.weight, incre_modules.3.0.bn2.bias, incre_modules.3.0.bn2.num_batches_tracked, incre_modules.3.0.bn2.running_mean, incre_modules.3.0.bn2.running_var, incre_modules.3.0.bn2.weight, incre_modules.3.0.bn3.bias, incre_modules.3.0.bn3.num_batches_tracked, incre_modules.3.0.bn3.running_mean, incre_modules.3.0.bn3.running_var, incre_modules.3.0.bn3.weight, incre_modules.3.0.conv1.weight, incre_modules.3.0.conv2.weight, incre_modules.3.0.conv3.weight, incre_modules.3.0.downsample.0.weight, incre_modules.3.0.downsample.1.bias, incre_modules.3.0.downsample.1.num_batches_tracked, incre_modules.3.0.downsample.1.running_mean, incre_modules.3.0.downsample.1.running_var, incre_modules.3.0.downsample.1.weight, classifier.bias, classifier.weight) found while loading pretrained weights. This may be expected if model is being adapted.
2024-07-23 13:17:07 Data Transforms:
2024-07-23 13:17:07   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2024-07-23 13:17:07   Validation: Compose([
  PadIfNeeded(always_apply=False, p=1.0, min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, position=PositionType.RANDOM, border_mode=4, value=None, mask_value=None),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2024-07-23 13:17:08 Using custom collate function: {'type': 'ResizeFromDataSizeCollate', 'min_scale': 0.4, 'max_scale': 1.0, 'min_short_side': 128, 'max_short_side': 1152, 'multiple_of': 32, 'to_square': False}
2024-07-23 13:17:08 
Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.
This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.
If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. 
This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).

2024-07-23 13:17:08 Using 114 images and 6 for testing
2024-07-23 13:17:08 
Starting pose model training...
--------------------------------------------------
2024-07-23 13:17:31 Training for epoch 1 done, starting evaluation
2024-07-23 13:17:32 Epoch 1 performance:
2024-07-23 13:17:32 metrics/test.rmse:  24.528
2024-07-23 13:17:32 metrics/test.rmse_pcutoff:18.649
2024-07-23 13:17:32 metrics/test.mAP:   0.561
2024-07-23 13:17:32 metrics/test.mAR:   1.667
2024-07-23 13:17:32 metrics/test.mAP_pcutoff:0.000
2024-07-23 13:17:32 metrics/test.mAR_pcutoff:0.000
2024-07-23 13:17:32 Epoch 1/200 (lr=0.0001), train loss 0.00998, valid loss 0.00654
2024-07-23 13:17:49 Training for epoch 2 done, starting evaluation
2024-07-23 13:17:49 Epoch 2 performance:
2024-07-23 13:17:49 metrics/test.rmse:  8.589
2024-07-23 13:17:49 metrics/test.rmse_pcutoff:4.370
2024-07-23 13:17:49 metrics/test.mAP:   22.525
2024-07-23 13:17:49 metrics/test.mAR:   30.000
2024-07-23 13:17:49 metrics/test.mAP_pcutoff:12.970
2024-07-23 13:17:49 metrics/test.mAR_pcutoff:16.667
2024-07-23 13:17:49 Epoch 2/200 (lr=0.0001), train loss 0.00512, valid loss 0.00539
2024-07-23 13:18:07 Training for epoch 3 done, starting evaluation
2024-07-23 13:18:08 Epoch 3 performance:
2024-07-23 13:18:08 metrics/test.rmse:  7.536
2024-07-23 13:18:08 metrics/test.rmse_pcutoff:8.001
2024-07-23 13:18:08 metrics/test.mAP:   44.530
2024-07-23 13:18:08 metrics/test.mAR:   45.000
2024-07-23 13:18:08 metrics/test.mAP_pcutoff:21.871
2024-07-23 13:18:08 metrics/test.mAR_pcutoff:25.000
2024-07-23 13:18:08 Epoch 3/200 (lr=0.0001), train loss 0.00398, valid loss 0.00334
2024-07-23 13:18:25 Training for epoch 4 done, starting evaluation
2024-07-23 13:18:25 Epoch 4 performance:
2024-07-23 13:18:25 metrics/test.rmse:  28.258
2024-07-23 13:18:25 metrics/test.rmse_pcutoff:28.258
2024-07-23 13:18:25 metrics/test.mAP:   28.936
2024-07-23 13:18:25 metrics/test.mAR:   38.333
2024-07-23 13:18:25 metrics/test.mAP_pcutoff:28.936
2024-07-23 13:18:25 metrics/test.mAR_pcutoff:38.333
2024-07-23 13:18:25 Epoch 4/200 (lr=0.0001), train loss 0.00365, valid loss 0.00327
2024-07-23 13:18:42 Training for epoch 5 done, starting evaluation
2024-07-23 13:18:43 Epoch 5 performance:
2024-07-23 13:18:43 metrics/test.rmse:  3.782
2024-07-23 13:18:43 metrics/test.rmse_pcutoff:3.782
2024-07-23 13:18:43 metrics/test.mAP:   52.713
2024-07-23 13:18:43 metrics/test.mAR:   53.333
2024-07-23 13:18:43 metrics/test.mAP_pcutoff:52.713
2024-07-23 13:18:43 metrics/test.mAR_pcutoff:53.333
2024-07-23 13:18:43 Epoch 5/200 (lr=0.0001), train loss 0.00258, valid loss 0.00291
2024-07-23 13:19:00 Training for epoch 6 done, starting evaluation
2024-07-23 13:19:00 Epoch 6 performance:
2024-07-23 13:19:00 metrics/test.rmse:  3.386
2024-07-23 13:19:00 metrics/test.rmse_pcutoff:3.386
2024-07-23 13:19:00 metrics/test.mAP:   60.619
2024-07-23 13:19:00 metrics/test.mAR:   63.333
2024-07-23 13:19:00 metrics/test.mAP_pcutoff:60.619
2024-07-23 13:19:00 metrics/test.mAR_pcutoff:63.333
2024-07-23 13:19:00 Epoch 6/200 (lr=0.0001), train loss 0.00217, valid loss 0.00242
2024-07-23 13:19:18 Training for epoch 7 done, starting evaluation
2024-07-23 13:19:19 Epoch 7 performance:
2024-07-23 13:19:19 metrics/test.rmse:  3.571
2024-07-23 13:19:19 metrics/test.rmse_pcutoff:3.412
2024-07-23 13:19:19 metrics/test.mAP:   60.743
2024-07-23 13:19:19 metrics/test.mAR:   61.667
2024-07-23 13:19:19 metrics/test.mAP_pcutoff:24.172
2024-07-23 13:19:19 metrics/test.mAR_pcutoff:25.000
2024-07-23 13:19:19 Epoch 7/200 (lr=0.0001), train loss 0.00221, valid loss 0.00227
2024-07-23 13:19:37 Training for epoch 8 done, starting evaluation
2024-07-23 13:19:38 Epoch 8 performance:
2024-07-23 13:19:38 metrics/test.rmse:  2.883
2024-07-23 13:19:38 metrics/test.rmse_pcutoff:2.883
2024-07-23 13:19:38 metrics/test.mAP:   63.267
2024-07-23 13:19:38 metrics/test.mAR:   65.000
2024-07-23 13:19:38 metrics/test.mAP_pcutoff:63.267
2024-07-23 13:19:38 metrics/test.mAR_pcutoff:65.000
2024-07-23 13:19:38 Epoch 8/200 (lr=0.0001), train loss 0.00190, valid loss 0.00167
2024-07-23 13:19:56 Training for epoch 9 done, starting evaluation
2024-07-23 13:19:56 Epoch 9 performance:
2024-07-23 13:19:56 metrics/test.rmse:  3.253
2024-07-23 13:19:56 metrics/test.rmse_pcutoff:3.253
2024-07-23 13:19:56 metrics/test.mAP:   59.446
2024-07-23 13:19:56 metrics/test.mAR:   61.667
2024-07-23 13:19:56 metrics/test.mAP_pcutoff:59.446
2024-07-23 13:19:56 metrics/test.mAR_pcutoff:61.667
2024-07-23 13:19:57 Epoch 9/200 (lr=0.0001), train loss 0.00161, valid loss 0.00184
2024-07-23 13:20:14 Training for epoch 10 done, starting evaluation
2024-07-23 13:20:15 Epoch 10 performance:
2024-07-23 13:20:15 metrics/test.rmse:  2.262
2024-07-23 13:20:15 metrics/test.rmse_pcutoff:2.262
2024-07-23 13:20:15 metrics/test.mAP:   76.733
2024-07-23 13:20:15 metrics/test.mAR:   78.333
2024-07-23 13:20:15 metrics/test.mAP_pcutoff:76.733
2024-07-23 13:20:15 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:20:15 Epoch 10/200 (lr=0.0001), train loss 0.00170, valid loss 0.00138
2024-07-23 13:20:32 Training for epoch 11 done, starting evaluation
2024-07-23 13:20:33 Epoch 11 performance:
2024-07-23 13:20:33 metrics/test.rmse:  2.458
2024-07-23 13:20:33 metrics/test.rmse_pcutoff:2.458
2024-07-23 13:20:33 metrics/test.mAP:   74.307
2024-07-23 13:20:33 metrics/test.mAR:   76.667
2024-07-23 13:20:33 metrics/test.mAP_pcutoff:74.307
2024-07-23 13:20:33 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:20:33 Epoch 11/200 (lr=0.0001), train loss 0.00156, valid loss 0.00188
2024-07-23 13:20:50 Training for epoch 12 done, starting evaluation
2024-07-23 13:20:50 Epoch 12 performance:
2024-07-23 13:20:50 metrics/test.rmse:  3.341
2024-07-23 13:20:50 metrics/test.rmse_pcutoff:3.271
2024-07-23 13:20:50 metrics/test.mAP:   57.574
2024-07-23 13:20:50 metrics/test.mAR:   60.000
2024-07-23 13:20:50 metrics/test.mAP_pcutoff:57.574
2024-07-23 13:20:50 metrics/test.mAR_pcutoff:60.000
2024-07-23 13:20:50 Epoch 12/200 (lr=0.0001), train loss 0.00157, valid loss 0.00185
2024-07-23 13:21:08 Training for epoch 13 done, starting evaluation
2024-07-23 13:21:09 Epoch 13 performance:
2024-07-23 13:21:09 metrics/test.rmse:  3.706
2024-07-23 13:21:09 metrics/test.rmse_pcutoff:3.706
2024-07-23 13:21:09 metrics/test.mAP:   47.054
2024-07-23 13:21:09 metrics/test.mAR:   50.000
2024-07-23 13:21:09 metrics/test.mAP_pcutoff:47.054
2024-07-23 13:21:09 metrics/test.mAR_pcutoff:50.000
2024-07-23 13:21:09 Epoch 13/200 (lr=0.0001), train loss 0.00135, valid loss 0.00212
2024-07-23 13:21:27 Training for epoch 14 done, starting evaluation
2024-07-23 13:21:28 Epoch 14 performance:
2024-07-23 13:21:28 metrics/test.rmse:  2.700
2024-07-23 13:21:28 metrics/test.rmse_pcutoff:2.700
2024-07-23 13:21:28 metrics/test.mAP:   71.361
2024-07-23 13:21:28 metrics/test.mAR:   73.333
2024-07-23 13:21:28 metrics/test.mAP_pcutoff:71.361
2024-07-23 13:21:28 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:21:28 Epoch 14/200 (lr=0.0001), train loss 0.00134, valid loss 0.00156
2024-07-23 13:21:46 Training for epoch 15 done, starting evaluation
2024-07-23 13:21:46 Epoch 15 performance:
2024-07-23 13:21:46 metrics/test.rmse:  3.306
2024-07-23 13:21:46 metrics/test.rmse_pcutoff:3.398
2024-07-23 13:21:46 metrics/test.mAP:   56.832
2024-07-23 13:21:46 metrics/test.mAR:   60.000
2024-07-23 13:21:46 metrics/test.mAP_pcutoff:56.832
2024-07-23 13:21:46 metrics/test.mAR_pcutoff:60.000
2024-07-23 13:21:46 Epoch 15/200 (lr=0.0001), train loss 0.00141, valid loss 0.00198
2024-07-23 13:22:07 Training for epoch 16 done, starting evaluation
2024-07-23 13:22:08 Epoch 16 performance:
2024-07-23 13:22:08 metrics/test.rmse:  3.117
2024-07-23 13:22:08 metrics/test.rmse_pcutoff:2.886
2024-07-23 13:22:08 metrics/test.mAP:   53.886
2024-07-23 13:22:08 metrics/test.mAR:   60.000
2024-07-23 13:22:08 metrics/test.mAP_pcutoff:29.545
2024-07-23 13:22:08 metrics/test.mAR_pcutoff:31.667
2024-07-23 13:22:08 Epoch 16/200 (lr=0.0001), train loss 0.00198, valid loss 0.00250
2024-07-23 13:22:26 Training for epoch 17 done, starting evaluation
2024-07-23 13:22:26 Epoch 17 performance:
2024-07-23 13:22:26 metrics/test.rmse:  3.976
2024-07-23 13:22:26 metrics/test.rmse_pcutoff:3.976
2024-07-23 13:22:26 metrics/test.mAP:   54.832
2024-07-23 13:22:26 metrics/test.mAR:   55.000
2024-07-23 13:22:26 metrics/test.mAP_pcutoff:54.832
2024-07-23 13:22:26 metrics/test.mAR_pcutoff:55.000
2024-07-23 13:22:26 Epoch 17/200 (lr=0.0001), train loss 0.00143, valid loss 0.00219
2024-07-23 13:22:45 Training for epoch 18 done, starting evaluation
2024-07-23 13:22:45 Epoch 18 performance:
2024-07-23 13:22:45 metrics/test.rmse:  3.619
2024-07-23 13:22:45 metrics/test.rmse_pcutoff:3.619
2024-07-23 13:22:45 metrics/test.mAP:   51.403
2024-07-23 13:22:45 metrics/test.mAR:   53.333
2024-07-23 13:22:45 metrics/test.mAP_pcutoff:51.403
2024-07-23 13:22:45 metrics/test.mAR_pcutoff:53.333
2024-07-23 13:22:45 Epoch 18/200 (lr=0.0001), train loss 0.00105, valid loss 0.00235
2024-07-23 13:23:03 Training for epoch 19 done, starting evaluation
2024-07-23 13:23:04 Epoch 19 performance:
2024-07-23 13:23:04 metrics/test.rmse:  5.015
2024-07-23 13:23:04 metrics/test.rmse_pcutoff:3.226
2024-07-23 13:23:04 metrics/test.mAP:   58.926
2024-07-23 13:23:04 metrics/test.mAR:   63.333
2024-07-23 13:23:04 metrics/test.mAP_pcutoff:42.488
2024-07-23 13:23:04 metrics/test.mAR_pcutoff:43.333
2024-07-23 13:23:04 Epoch 19/200 (lr=0.0001), train loss 0.00109, valid loss 0.00226
2024-07-23 13:23:22 Training for epoch 20 done, starting evaluation
2024-07-23 13:23:22 Epoch 20 performance:
2024-07-23 13:23:22 metrics/test.rmse:  2.672
2024-07-23 13:23:22 metrics/test.rmse_pcutoff:2.672
2024-07-23 13:23:22 metrics/test.mAP:   70.875
2024-07-23 13:23:22 metrics/test.mAR:   75.000
2024-07-23 13:23:22 metrics/test.mAP_pcutoff:70.875
2024-07-23 13:23:22 metrics/test.mAR_pcutoff:75.000
2024-07-23 13:23:22 Epoch 20/200 (lr=0.0001), train loss 0.00112, valid loss 0.00179
2024-07-23 13:23:40 Training for epoch 21 done, starting evaluation
2024-07-23 13:23:41 Epoch 21 performance:
2024-07-23 13:23:41 metrics/test.rmse:  2.767
2024-07-23 13:23:41 metrics/test.rmse_pcutoff:2.767
2024-07-23 13:23:41 metrics/test.mAP:   66.312
2024-07-23 13:23:41 metrics/test.mAR:   68.333
2024-07-23 13:23:41 metrics/test.mAP_pcutoff:66.312
2024-07-23 13:23:41 metrics/test.mAR_pcutoff:68.333
2024-07-23 13:23:41 Epoch 21/200 (lr=0.0001), train loss 0.00106, valid loss 0.00134
2024-07-23 13:23:58 Training for epoch 22 done, starting evaluation
2024-07-23 13:23:59 Epoch 22 performance:
2024-07-23 13:23:59 metrics/test.rmse:  3.099
2024-07-23 13:23:59 metrics/test.rmse_pcutoff:3.099
2024-07-23 13:23:59 metrics/test.mAP:   60.322
2024-07-23 13:23:59 metrics/test.mAR:   63.333
2024-07-23 13:23:59 metrics/test.mAP_pcutoff:60.322
2024-07-23 13:23:59 metrics/test.mAR_pcutoff:63.333
2024-07-23 13:23:59 Epoch 22/200 (lr=0.0001), train loss 0.00093, valid loss 0.00145
2024-07-23 13:24:16 Training for epoch 23 done, starting evaluation
2024-07-23 13:24:16 Epoch 23 performance:
2024-07-23 13:24:16 metrics/test.rmse:  2.983
2024-07-23 13:24:16 metrics/test.rmse_pcutoff:2.983
2024-07-23 13:24:16 metrics/test.mAP:   66.955
2024-07-23 13:24:16 metrics/test.mAR:   68.333
2024-07-23 13:24:16 metrics/test.mAP_pcutoff:66.955
2024-07-23 13:24:16 metrics/test.mAR_pcutoff:68.333
2024-07-23 13:24:17 Epoch 23/200 (lr=0.0001), train loss 0.00102, valid loss 0.00172
2024-07-23 13:24:35 Training for epoch 24 done, starting evaluation
2024-07-23 13:24:35 Epoch 24 performance:
2024-07-23 13:24:35 metrics/test.rmse:  2.502
2024-07-23 13:24:35 metrics/test.rmse_pcutoff:2.502
2024-07-23 13:24:35 metrics/test.mAP:   75.569
2024-07-23 13:24:35 metrics/test.mAR:   76.667
2024-07-23 13:24:35 metrics/test.mAP_pcutoff:75.569
2024-07-23 13:24:35 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:24:35 Epoch 24/200 (lr=0.0001), train loss 0.00070, valid loss 0.00121
2024-07-23 13:24:55 Training for epoch 25 done, starting evaluation
2024-07-23 13:24:55 Epoch 25 performance:
2024-07-23 13:24:55 metrics/test.rmse:  3.126
2024-07-23 13:24:55 metrics/test.rmse_pcutoff:3.126
2024-07-23 13:24:55 metrics/test.mAP:   60.322
2024-07-23 13:24:55 metrics/test.mAR:   61.667
2024-07-23 13:24:55 metrics/test.mAP_pcutoff:60.322
2024-07-23 13:24:55 metrics/test.mAR_pcutoff:61.667
2024-07-23 13:24:55 Epoch 25/200 (lr=0.0001), train loss 0.00093, valid loss 0.00157
2024-07-23 13:25:15 Training for epoch 26 done, starting evaluation
2024-07-23 13:25:15 Epoch 26 performance:
2024-07-23 13:25:15 metrics/test.rmse:  3.271
2024-07-23 13:25:15 metrics/test.rmse_pcutoff:3.271
2024-07-23 13:25:15 metrics/test.mAP:   60.322
2024-07-23 13:25:15 metrics/test.mAR:   61.667
2024-07-23 13:25:15 metrics/test.mAP_pcutoff:60.322
2024-07-23 13:25:15 metrics/test.mAR_pcutoff:61.667
2024-07-23 13:25:15 Epoch 26/200 (lr=0.0001), train loss 0.00094, valid loss 0.00195
2024-07-23 13:25:34 Training for epoch 27 done, starting evaluation
2024-07-23 13:25:35 Epoch 27 performance:
2024-07-23 13:25:35 metrics/test.rmse:  2.993
2024-07-23 13:25:35 metrics/test.rmse_pcutoff:2.993
2024-07-23 13:25:35 metrics/test.mAP:   61.163
2024-07-23 13:25:35 metrics/test.mAR:   63.333
2024-07-23 13:25:35 metrics/test.mAP_pcutoff:61.163
2024-07-23 13:25:35 metrics/test.mAR_pcutoff:63.333
2024-07-23 13:25:35 Epoch 27/200 (lr=0.0001), train loss 0.00067, valid loss 0.00147
2024-07-23 13:25:53 Training for epoch 28 done, starting evaluation
2024-07-23 13:25:54 Epoch 28 performance:
2024-07-23 13:25:54 metrics/test.rmse:  2.552
2024-07-23 13:25:54 metrics/test.rmse_pcutoff:2.552
2024-07-23 13:25:54 metrics/test.mAP:   70.842
2024-07-23 13:25:54 metrics/test.mAR:   73.333
2024-07-23 13:25:54 metrics/test.mAP_pcutoff:70.842
2024-07-23 13:25:54 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:25:54 Epoch 28/200 (lr=0.0001), train loss 0.00063, valid loss 0.00131
2024-07-23 13:26:12 Training for epoch 29 done, starting evaluation
2024-07-23 13:26:12 Epoch 29 performance:
2024-07-23 13:26:12 metrics/test.rmse:  2.662
2024-07-23 13:26:12 metrics/test.rmse_pcutoff:2.662
2024-07-23 13:26:12 metrics/test.mAP:   75.330
2024-07-23 13:26:12 metrics/test.mAR:   76.667
2024-07-23 13:26:12 metrics/test.mAP_pcutoff:75.330
2024-07-23 13:26:12 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:26:12 Epoch 29/200 (lr=0.0001), train loss 0.00072, valid loss 0.00124
2024-07-23 13:26:31 Training for epoch 30 done, starting evaluation
2024-07-23 13:26:31 Epoch 30 performance:
2024-07-23 13:26:31 metrics/test.rmse:  2.632
2024-07-23 13:26:31 metrics/test.rmse_pcutoff:2.632
2024-07-23 13:26:31 metrics/test.mAP:   73.267
2024-07-23 13:26:31 metrics/test.mAR:   73.333
2024-07-23 13:26:31 metrics/test.mAP_pcutoff:73.267
2024-07-23 13:26:31 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:26:31 Epoch 30/200 (lr=0.0001), train loss 0.00072, valid loss 0.00123
2024-07-23 13:26:50 Training for epoch 31 done, starting evaluation
2024-07-23 13:26:50 Epoch 31 performance:
2024-07-23 13:26:50 metrics/test.rmse:  2.532
2024-07-23 13:26:50 metrics/test.rmse_pcutoff:2.532
2024-07-23 13:26:50 metrics/test.mAP:   76.733
2024-07-23 13:26:50 metrics/test.mAR:   78.333
2024-07-23 13:26:50 metrics/test.mAP_pcutoff:76.733
2024-07-23 13:26:50 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:26:50 Epoch 31/200 (lr=0.0001), train loss 0.00072, valid loss 0.00118
2024-07-23 13:27:08 Training for epoch 32 done, starting evaluation
2024-07-23 13:27:08 Epoch 32 performance:
2024-07-23 13:27:08 metrics/test.rmse:  2.612
2024-07-23 13:27:08 metrics/test.rmse_pcutoff:2.612
2024-07-23 13:27:08 metrics/test.mAP:   71.361
2024-07-23 13:27:08 metrics/test.mAR:   73.333
2024-07-23 13:27:08 metrics/test.mAP_pcutoff:71.361
2024-07-23 13:27:08 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:27:08 Epoch 32/200 (lr=0.0001), train loss 0.00070, valid loss 0.00141
2024-07-23 13:27:26 Training for epoch 33 done, starting evaluation
2024-07-23 13:27:27 Epoch 33 performance:
2024-07-23 13:27:27 metrics/test.rmse:  2.721
2024-07-23 13:27:27 metrics/test.rmse_pcutoff:2.742
2024-07-23 13:27:27 metrics/test.mAP:   72.203
2024-07-23 13:27:27 metrics/test.mAR:   73.333
2024-07-23 13:27:27 metrics/test.mAP_pcutoff:65.470
2024-07-23 13:27:27 metrics/test.mAR_pcutoff:66.667
2024-07-23 13:27:27 Epoch 33/200 (lr=0.0001), train loss 0.00059, valid loss 0.00126
2024-07-23 13:27:45 Training for epoch 34 done, starting evaluation
2024-07-23 13:27:46 Epoch 34 performance:
2024-07-23 13:27:46 metrics/test.rmse:  2.727
2024-07-23 13:27:46 metrics/test.rmse_pcutoff:2.727
2024-07-23 13:27:46 metrics/test.mAP:   67.475
2024-07-23 13:27:46 metrics/test.mAR:   70.000
2024-07-23 13:27:46 metrics/test.mAP_pcutoff:67.475
2024-07-23 13:27:46 metrics/test.mAR_pcutoff:70.000
2024-07-23 13:27:46 Epoch 34/200 (lr=0.0001), train loss 0.00063, valid loss 0.00128
2024-07-23 13:28:04 Training for epoch 35 done, starting evaluation
2024-07-23 13:28:05 Epoch 35 performance:
2024-07-23 13:28:05 metrics/test.rmse:  2.389
2024-07-23 13:28:05 metrics/test.rmse_pcutoff:2.389
2024-07-23 13:28:05 metrics/test.mAP:   79.257
2024-07-23 13:28:05 metrics/test.mAR:   80.000
2024-07-23 13:28:05 metrics/test.mAP_pcutoff:79.257
2024-07-23 13:28:05 metrics/test.mAR_pcutoff:80.000
2024-07-23 13:28:05 Epoch 35/200 (lr=0.0001), train loss 0.00075, valid loss 0.00102
2024-07-23 13:28:24 Training for epoch 36 done, starting evaluation
2024-07-23 13:28:24 Epoch 36 performance:
2024-07-23 13:28:24 metrics/test.rmse:  8.494
2024-07-23 13:28:24 metrics/test.rmse_pcutoff:8.550
2024-07-23 13:28:24 metrics/test.mAP:   29.876
2024-07-23 13:28:24 metrics/test.mAR:   35.000
2024-07-23 13:28:24 metrics/test.mAP_pcutoff:2.104
2024-07-23 13:28:24 metrics/test.mAR_pcutoff:5.000
2024-07-23 13:28:24 Epoch 36/200 (lr=0.0001), train loss 0.00113, valid loss 0.00349
2024-07-23 13:28:42 Training for epoch 37 done, starting evaluation
2024-07-23 13:28:42 Epoch 37 performance:
2024-07-23 13:28:42 metrics/test.rmse:  3.110
2024-07-23 13:28:42 metrics/test.rmse_pcutoff:3.110
2024-07-23 13:28:42 metrics/test.mAP:   64.109
2024-07-23 13:28:42 metrics/test.mAR:   65.000
2024-07-23 13:28:42 metrics/test.mAP_pcutoff:64.109
2024-07-23 13:28:42 metrics/test.mAR_pcutoff:65.000
2024-07-23 13:28:42 Epoch 37/200 (lr=0.0001), train loss 0.00133, valid loss 0.00167
2024-07-23 13:29:02 Training for epoch 38 done, starting evaluation
2024-07-23 13:29:02 Epoch 38 performance:
2024-07-23 13:29:02 metrics/test.rmse:  2.777
2024-07-23 13:29:02 metrics/test.rmse_pcutoff:2.777
2024-07-23 13:29:02 metrics/test.mAP:   66.213
2024-07-23 13:29:02 metrics/test.mAR:   68.333
2024-07-23 13:29:02 metrics/test.mAP_pcutoff:66.213
2024-07-23 13:29:02 metrics/test.mAR_pcutoff:68.333
2024-07-23 13:29:02 Epoch 38/200 (lr=0.0001), train loss 0.00081, valid loss 0.00142
2024-07-23 13:29:22 Training for epoch 39 done, starting evaluation
2024-07-23 13:29:22 Epoch 39 performance:
2024-07-23 13:29:22 metrics/test.rmse:  2.739
2024-07-23 13:29:22 metrics/test.rmse_pcutoff:2.739
2024-07-23 13:29:22 metrics/test.mAP:   69.257
2024-07-23 13:29:22 metrics/test.mAR:   71.667
2024-07-23 13:29:22 metrics/test.mAP_pcutoff:69.257
2024-07-23 13:29:22 metrics/test.mAR_pcutoff:71.667
2024-07-23 13:29:22 Epoch 39/200 (lr=0.0001), train loss 0.00077, valid loss 0.00192
2024-07-23 13:29:41 Training for epoch 40 done, starting evaluation
2024-07-23 13:29:42 Epoch 40 performance:
2024-07-23 13:29:42 metrics/test.rmse:  3.255
2024-07-23 13:29:42 metrics/test.rmse_pcutoff:3.255
2024-07-23 13:29:42 metrics/test.mAP:   53.168
2024-07-23 13:29:42 metrics/test.mAR:   55.000
2024-07-23 13:29:42 metrics/test.mAP_pcutoff:53.168
2024-07-23 13:29:42 metrics/test.mAR_pcutoff:55.000
2024-07-23 13:29:42 Epoch 40/200 (lr=0.0001), train loss 0.00066, valid loss 0.00162
2024-07-23 13:30:01 Training for epoch 41 done, starting evaluation
2024-07-23 13:30:01 Epoch 41 performance:
2024-07-23 13:30:01 metrics/test.rmse:  3.089
2024-07-23 13:30:01 metrics/test.rmse_pcutoff:3.089
2024-07-23 13:30:01 metrics/test.mAP:   67.558
2024-07-23 13:30:01 metrics/test.mAR:   70.000
2024-07-23 13:30:01 metrics/test.mAP_pcutoff:67.558
2024-07-23 13:30:01 metrics/test.mAR_pcutoff:70.000
2024-07-23 13:30:01 Epoch 41/200 (lr=0.0001), train loss 0.00096, valid loss 0.00170
2024-07-23 13:30:19 Training for epoch 42 done, starting evaluation
2024-07-23 13:30:20 Epoch 42 performance:
2024-07-23 13:30:20 metrics/test.rmse:  2.942
2024-07-23 13:30:20 metrics/test.rmse_pcutoff:2.942
2024-07-23 13:30:20 metrics/test.mAP:   66.634
2024-07-23 13:30:20 metrics/test.mAR:   68.333
2024-07-23 13:30:20 metrics/test.mAP_pcutoff:66.634
2024-07-23 13:30:20 metrics/test.mAR_pcutoff:68.333
2024-07-23 13:30:20 Epoch 42/200 (lr=0.0001), train loss 0.00073, valid loss 0.00153
2024-07-23 13:30:37 Training for epoch 43 done, starting evaluation
2024-07-23 13:30:38 Epoch 43 performance:
2024-07-23 13:30:38 metrics/test.rmse:  2.648
2024-07-23 13:30:38 metrics/test.rmse_pcutoff:2.648
2024-07-23 13:30:38 metrics/test.mAP:   74.909
2024-07-23 13:30:38 metrics/test.mAR:   76.667
2024-07-23 13:30:38 metrics/test.mAP_pcutoff:74.909
2024-07-23 13:30:38 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:30:38 Epoch 43/200 (lr=0.0001), train loss 0.00061, valid loss 0.00114
2024-07-23 13:30:56 Training for epoch 44 done, starting evaluation
2024-07-23 13:30:56 Epoch 44 performance:
2024-07-23 13:30:56 metrics/test.rmse:  2.955
2024-07-23 13:30:56 metrics/test.rmse_pcutoff:2.955
2024-07-23 13:30:56 metrics/test.mAP:   69.373
2024-07-23 13:30:56 metrics/test.mAR:   73.333
2024-07-23 13:30:56 metrics/test.mAP_pcutoff:69.373
2024-07-23 13:30:56 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:30:56 Epoch 44/200 (lr=0.0001), train loss 0.00060, valid loss 0.00136
2024-07-23 13:31:15 Training for epoch 45 done, starting evaluation
2024-07-23 13:31:15 Epoch 45 performance:
2024-07-23 13:31:15 metrics/test.rmse:  3.686
2024-07-23 13:31:15 metrics/test.rmse_pcutoff:3.686
2024-07-23 13:31:15 metrics/test.mAP:   54.150
2024-07-23 13:31:15 metrics/test.mAR:   58.333
2024-07-23 13:31:15 metrics/test.mAP_pcutoff:54.150
2024-07-23 13:31:15 metrics/test.mAR_pcutoff:58.333
2024-07-23 13:31:15 Epoch 45/200 (lr=0.0001), train loss 0.00062, valid loss 0.00259
2024-07-23 13:31:34 Training for epoch 46 done, starting evaluation
2024-07-23 13:31:34 Epoch 46 performance:
2024-07-23 13:31:34 metrics/test.rmse:  2.843
2024-07-23 13:31:34 metrics/test.rmse_pcutoff:2.843
2024-07-23 13:31:34 metrics/test.mAP:   64.950
2024-07-23 13:31:34 metrics/test.mAR:   68.333
2024-07-23 13:31:34 metrics/test.mAP_pcutoff:64.950
2024-07-23 13:31:34 metrics/test.mAR_pcutoff:68.333
2024-07-23 13:31:34 Epoch 46/200 (lr=0.0001), train loss 0.00062, valid loss 0.00137
2024-07-23 13:31:54 Training for epoch 47 done, starting evaluation
2024-07-23 13:31:54 Epoch 47 performance:
2024-07-23 13:31:54 metrics/test.rmse:  2.674
2024-07-23 13:31:54 metrics/test.rmse_pcutoff:2.674
2024-07-23 13:31:54 metrics/test.mAP:   74.348
2024-07-23 13:31:54 metrics/test.mAR:   75.000
2024-07-23 13:31:54 metrics/test.mAP_pcutoff:74.348
2024-07-23 13:31:54 metrics/test.mAR_pcutoff:75.000
2024-07-23 13:31:54 Epoch 47/200 (lr=0.0001), train loss 0.00054, valid loss 0.00123
2024-07-23 13:32:13 Training for epoch 48 done, starting evaluation
2024-07-23 13:32:13 Epoch 48 performance:
2024-07-23 13:32:13 metrics/test.rmse:  2.823
2024-07-23 13:32:13 metrics/test.rmse_pcutoff:2.823
2024-07-23 13:32:13 metrics/test.mAP:   69.158
2024-07-23 13:32:13 metrics/test.mAR:   70.000
2024-07-23 13:32:13 metrics/test.mAP_pcutoff:69.158
2024-07-23 13:32:13 metrics/test.mAR_pcutoff:70.000
2024-07-23 13:32:13 Epoch 48/200 (lr=0.0001), train loss 0.00065, valid loss 0.00129
2024-07-23 13:32:32 Training for epoch 49 done, starting evaluation
2024-07-23 13:32:33 Epoch 49 performance:
2024-07-23 13:32:33 metrics/test.rmse:  2.249
2024-07-23 13:32:33 metrics/test.rmse_pcutoff:2.249
2024-07-23 13:32:33 metrics/test.mAP:   83.366
2024-07-23 13:32:33 metrics/test.mAR:   83.333
2024-07-23 13:32:33 metrics/test.mAP_pcutoff:83.366
2024-07-23 13:32:33 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:32:33 Epoch 49/200 (lr=0.0001), train loss 0.00060, valid loss 0.00094
2024-07-23 13:32:51 Training for epoch 50 done, starting evaluation
2024-07-23 13:32:51 Epoch 50 performance:
2024-07-23 13:32:51 metrics/test.rmse:  2.229
2024-07-23 13:32:51 metrics/test.rmse_pcutoff:2.229
2024-07-23 13:32:51 metrics/test.mAP:   81.378
2024-07-23 13:32:51 metrics/test.mAR:   83.333
2024-07-23 13:32:51 metrics/test.mAP_pcutoff:81.378
2024-07-23 13:32:51 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:32:52 Epoch 50/200 (lr=0.0001), train loss 0.00062, valid loss 0.00098
2024-07-23 13:33:10 Training for epoch 51 done, starting evaluation
2024-07-23 13:33:10 Epoch 51 performance:
2024-07-23 13:33:10 metrics/test.rmse:  2.710
2024-07-23 13:33:10 metrics/test.rmse_pcutoff:2.710
2024-07-23 13:33:10 metrics/test.mAP:   71.081
2024-07-23 13:33:10 metrics/test.mAR:   73.333
2024-07-23 13:33:10 metrics/test.mAP_pcutoff:71.081
2024-07-23 13:33:10 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:33:10 Epoch 51/200 (lr=0.0001), train loss 0.00047, valid loss 0.00154
2024-07-23 13:33:28 Training for epoch 52 done, starting evaluation
2024-07-23 13:33:29 Epoch 52 performance:
2024-07-23 13:33:29 metrics/test.rmse:  2.653
2024-07-23 13:33:29 metrics/test.rmse_pcutoff:2.653
2024-07-23 13:33:29 metrics/test.mAP:   71.163
2024-07-23 13:33:29 metrics/test.mAR:   71.667
2024-07-23 13:33:29 metrics/test.mAP_pcutoff:71.163
2024-07-23 13:33:29 metrics/test.mAR_pcutoff:71.667
2024-07-23 13:33:29 Epoch 52/200 (lr=0.0001), train loss 0.00051, valid loss 0.00118
2024-07-23 13:33:47 Training for epoch 53 done, starting evaluation
2024-07-23 13:33:47 Epoch 53 performance:
2024-07-23 13:33:47 metrics/test.rmse:  2.748
2024-07-23 13:33:47 metrics/test.rmse_pcutoff:2.748
2024-07-23 13:33:47 metrics/test.mAP:   66.394
2024-07-23 13:33:47 metrics/test.mAR:   70.000
2024-07-23 13:33:47 metrics/test.mAP_pcutoff:66.394
2024-07-23 13:33:47 metrics/test.mAR_pcutoff:70.000
2024-07-23 13:33:47 Epoch 53/200 (lr=0.0001), train loss 0.00050, valid loss 0.00138
2024-07-23 13:34:07 Training for epoch 54 done, starting evaluation
2024-07-23 13:34:07 Epoch 54 performance:
2024-07-23 13:34:07 metrics/test.rmse:  2.027
2024-07-23 13:34:07 metrics/test.rmse_pcutoff:2.027
2024-07-23 13:34:07 metrics/test.mAP:   84.629
2024-07-23 13:34:07 metrics/test.mAR:   86.667
2024-07-23 13:34:07 metrics/test.mAP_pcutoff:84.629
2024-07-23 13:34:07 metrics/test.mAR_pcutoff:86.667
2024-07-23 13:34:07 Epoch 54/200 (lr=0.0001), train loss 0.00051, valid loss 0.00091
2024-07-23 13:34:26 Training for epoch 55 done, starting evaluation
2024-07-23 13:34:27 Epoch 55 performance:
2024-07-23 13:34:27 metrics/test.rmse:  2.294
2024-07-23 13:34:27 metrics/test.rmse_pcutoff:2.294
2024-07-23 13:34:27 metrics/test.mAP:   81.030
2024-07-23 13:34:27 metrics/test.mAR:   83.333
2024-07-23 13:34:27 metrics/test.mAP_pcutoff:81.030
2024-07-23 13:34:27 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:34:27 Epoch 55/200 (lr=0.0001), train loss 0.00049, valid loss 0.00091
2024-07-23 13:34:44 Training for epoch 56 done, starting evaluation
2024-07-23 13:34:45 Epoch 56 performance:
2024-07-23 13:34:45 metrics/test.rmse:  2.293
2024-07-23 13:34:45 metrics/test.rmse_pcutoff:2.293
2024-07-23 13:34:45 metrics/test.mAP:   79.901
2024-07-23 13:34:45 metrics/test.mAR:   80.000
2024-07-23 13:34:45 metrics/test.mAP_pcutoff:79.901
2024-07-23 13:34:45 metrics/test.mAR_pcutoff:80.000
2024-07-23 13:34:45 Epoch 56/200 (lr=0.0001), train loss 0.00043, valid loss 0.00073
2024-07-23 13:35:02 Training for epoch 57 done, starting evaluation
2024-07-23 13:35:02 Epoch 57 performance:
2024-07-23 13:35:02 metrics/test.rmse:  2.320
2024-07-23 13:35:02 metrics/test.rmse_pcutoff:2.320
2024-07-23 13:35:02 metrics/test.mAP:   77.195
2024-07-23 13:35:02 metrics/test.mAR:   80.000
2024-07-23 13:35:02 metrics/test.mAP_pcutoff:77.195
2024-07-23 13:35:02 metrics/test.mAR_pcutoff:80.000
2024-07-23 13:35:02 Epoch 57/200 (lr=0.0001), train loss 0.00041, valid loss 0.00104
2024-07-23 13:35:20 Training for epoch 58 done, starting evaluation
2024-07-23 13:35:20 Epoch 58 performance:
2024-07-23 13:35:20 metrics/test.rmse:  2.057
2024-07-23 13:35:20 metrics/test.rmse_pcutoff:2.057
2024-07-23 13:35:20 metrics/test.mAP:   82.071
2024-07-23 13:35:20 metrics/test.mAR:   85.000
2024-07-23 13:35:20 metrics/test.mAP_pcutoff:82.071
2024-07-23 13:35:20 metrics/test.mAR_pcutoff:85.000
2024-07-23 13:35:20 Epoch 58/200 (lr=0.0001), train loss 0.00050, valid loss 0.00097
2024-07-23 13:35:39 Training for epoch 59 done, starting evaluation
2024-07-23 13:35:39 Epoch 59 performance:
2024-07-23 13:35:39 metrics/test.rmse:  2.172
2024-07-23 13:35:39 metrics/test.rmse_pcutoff:2.172
2024-07-23 13:35:39 metrics/test.mAP:   86.634
2024-07-23 13:35:39 metrics/test.mAR:   86.667
2024-07-23 13:35:39 metrics/test.mAP_pcutoff:86.634
2024-07-23 13:35:39 metrics/test.mAR_pcutoff:86.667
2024-07-23 13:35:39 Epoch 59/200 (lr=0.0001), train loss 0.00046, valid loss 0.00073
2024-07-23 13:35:57 Training for epoch 60 done, starting evaluation
2024-07-23 13:35:57 Epoch 60 performance:
2024-07-23 13:35:57 metrics/test.rmse:  5.519
2024-07-23 13:35:57 metrics/test.rmse_pcutoff:3.423
2024-07-23 13:35:57 metrics/test.mAP:   53.267
2024-07-23 13:35:57 metrics/test.mAR:   55.000
2024-07-23 13:35:57 metrics/test.mAP_pcutoff:44.343
2024-07-23 13:35:57 metrics/test.mAR_pcutoff:46.667
2024-07-23 13:35:57 Epoch 60/200 (lr=0.0001), train loss 0.00078, valid loss 0.00341
2024-07-23 13:36:16 Training for epoch 61 done, starting evaluation
2024-07-23 13:36:16 Epoch 61 performance:
2024-07-23 13:36:16 metrics/test.rmse:  2.840
2024-07-23 13:36:16 metrics/test.rmse_pcutoff:2.746
2024-07-23 13:36:16 metrics/test.mAP:   63.134
2024-07-23 13:36:16 metrics/test.mAR:   66.667
2024-07-23 13:36:16 metrics/test.mAP_pcutoff:52.805
2024-07-23 13:36:16 metrics/test.mAR_pcutoff:56.667
2024-07-23 13:36:16 Epoch 61/200 (lr=0.0001), train loss 0.00198, valid loss 0.00163
2024-07-23 13:36:37 Training for epoch 62 done, starting evaluation
2024-07-23 13:36:37 Epoch 62 performance:
2024-07-23 13:36:37 metrics/test.rmse:  2.533
2024-07-23 13:36:37 metrics/test.rmse_pcutoff:2.533
2024-07-23 13:36:37 metrics/test.mAP:   67.153
2024-07-23 13:36:37 metrics/test.mAR:   70.000
2024-07-23 13:36:37 metrics/test.mAP_pcutoff:67.153
2024-07-23 13:36:37 metrics/test.mAR_pcutoff:70.000
2024-07-23 13:36:37 Epoch 62/200 (lr=0.0001), train loss 0.00100, valid loss 0.00119
2024-07-23 13:36:56 Training for epoch 63 done, starting evaluation
2024-07-23 13:36:56 Epoch 63 performance:
2024-07-23 13:36:56 metrics/test.rmse:  2.731
2024-07-23 13:36:56 metrics/test.rmse_pcutoff:2.731
2024-07-23 13:36:56 metrics/test.mAP:   73.366
2024-07-23 13:36:56 metrics/test.mAR:   75.000
2024-07-23 13:36:56 metrics/test.mAP_pcutoff:73.366
2024-07-23 13:36:56 metrics/test.mAR_pcutoff:75.000
2024-07-23 13:36:56 Epoch 63/200 (lr=0.0001), train loss 0.00054, valid loss 0.00117
2024-07-23 13:37:16 Training for epoch 64 done, starting evaluation
2024-07-23 13:37:17 Epoch 64 performance:
2024-07-23 13:37:17 metrics/test.rmse:  2.714
2024-07-23 13:37:17 metrics/test.rmse_pcutoff:2.714
2024-07-23 13:37:17 metrics/test.mAP:   76.749
2024-07-23 13:37:17 metrics/test.mAR:   78.333
2024-07-23 13:37:17 metrics/test.mAP_pcutoff:76.749
2024-07-23 13:37:17 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:37:17 Epoch 64/200 (lr=0.0001), train loss 0.00047, valid loss 0.00114
2024-07-23 13:37:37 Training for epoch 65 done, starting evaluation
2024-07-23 13:37:38 Epoch 65 performance:
2024-07-23 13:37:38 metrics/test.rmse:  2.220
2024-07-23 13:37:38 metrics/test.rmse_pcutoff:2.220
2024-07-23 13:37:38 metrics/test.mAP:   82.838
2024-07-23 13:37:38 metrics/test.mAR:   85.000
2024-07-23 13:37:38 metrics/test.mAP_pcutoff:82.838
2024-07-23 13:37:38 metrics/test.mAR_pcutoff:85.000
2024-07-23 13:37:38 Epoch 65/200 (lr=0.0001), train loss 0.00046, valid loss 0.00077
2024-07-23 13:37:58 Training for epoch 66 done, starting evaluation
2024-07-23 13:37:58 Epoch 66 performance:
2024-07-23 13:37:58 metrics/test.rmse:  2.471
2024-07-23 13:37:58 metrics/test.rmse_pcutoff:2.471
2024-07-23 13:37:58 metrics/test.mAP:   74.307
2024-07-23 13:37:58 metrics/test.mAR:   76.667
2024-07-23 13:37:58 metrics/test.mAP_pcutoff:74.307
2024-07-23 13:37:58 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:37:58 Epoch 66/200 (lr=0.0001), train loss 0.00043, valid loss 0.00110
2024-07-23 13:38:17 Training for epoch 67 done, starting evaluation
2024-07-23 13:38:18 Epoch 67 performance:
2024-07-23 13:38:18 metrics/test.rmse:  2.572
2024-07-23 13:38:18 metrics/test.rmse_pcutoff:2.572
2024-07-23 13:38:18 metrics/test.mAP:   73.606
2024-07-23 13:38:18 metrics/test.mAR:   76.667
2024-07-23 13:38:18 metrics/test.mAP_pcutoff:73.606
2024-07-23 13:38:18 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:38:18 Epoch 67/200 (lr=0.0001), train loss 0.00046, valid loss 0.00100
2024-07-23 13:38:37 Training for epoch 68 done, starting evaluation
2024-07-23 13:38:38 Epoch 68 performance:
2024-07-23 13:38:38 metrics/test.rmse:  2.381
2024-07-23 13:38:38 metrics/test.rmse_pcutoff:2.381
2024-07-23 13:38:38 metrics/test.mAP:   77.756
2024-07-23 13:38:38 metrics/test.mAR:   80.000
2024-07-23 13:38:38 metrics/test.mAP_pcutoff:77.756
2024-07-23 13:38:38 metrics/test.mAR_pcutoff:80.000
2024-07-23 13:38:38 Epoch 68/200 (lr=0.0001), train loss 0.00038, valid loss 0.00109
2024-07-23 13:38:57 Training for epoch 69 done, starting evaluation
2024-07-23 13:38:58 Epoch 69 performance:
2024-07-23 13:38:58 metrics/test.rmse:  2.590
2024-07-23 13:38:58 metrics/test.rmse_pcutoff:2.560
2024-07-23 13:38:58 metrics/test.mAP:   71.163
2024-07-23 13:38:58 metrics/test.mAR:   73.333
2024-07-23 13:38:58 metrics/test.mAP_pcutoff:61.064
2024-07-23 13:38:58 metrics/test.mAR_pcutoff:63.333
2024-07-23 13:38:58 Epoch 69/200 (lr=0.0001), train loss 0.00075, valid loss 0.00157
2024-07-23 13:39:18 Training for epoch 70 done, starting evaluation
2024-07-23 13:39:18 Epoch 70 performance:
2024-07-23 13:39:18 metrics/test.rmse:  2.922
2024-07-23 13:39:18 metrics/test.rmse_pcutoff:2.922
2024-07-23 13:39:18 metrics/test.mAP:   69.084
2024-07-23 13:39:18 metrics/test.mAR:   73.333
2024-07-23 13:39:18 metrics/test.mAP_pcutoff:69.084
2024-07-23 13:39:18 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:39:18 Epoch 70/200 (lr=0.0001), train loss 0.00070, valid loss 0.00193
2024-07-23 13:39:38 Training for epoch 71 done, starting evaluation
2024-07-23 13:39:38 Epoch 71 performance:
2024-07-23 13:39:38 metrics/test.rmse:  2.620
2024-07-23 13:39:38 metrics/test.rmse_pcutoff:2.620
2024-07-23 13:39:38 metrics/test.mAP:   75.371
2024-07-23 13:39:38 metrics/test.mAR:   78.333
2024-07-23 13:39:38 metrics/test.mAP_pcutoff:75.371
2024-07-23 13:39:38 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:39:38 Epoch 71/200 (lr=0.0001), train loss 0.00061, valid loss 0.00110
2024-07-23 13:39:58 Training for epoch 72 done, starting evaluation
2024-07-23 13:39:59 Epoch 72 performance:
2024-07-23 13:39:59 metrics/test.rmse:  2.595
2024-07-23 13:39:59 metrics/test.rmse_pcutoff:2.595
2024-07-23 13:39:59 metrics/test.mAP:   76.172
2024-07-23 13:39:59 metrics/test.mAR:   78.333
2024-07-23 13:39:59 metrics/test.mAP_pcutoff:76.172
2024-07-23 13:39:59 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:39:59 Epoch 72/200 (lr=0.0001), train loss 0.00064, valid loss 0.00123
2024-07-23 13:40:28 Training for epoch 73 done, starting evaluation
2024-07-23 13:40:29 Epoch 73 performance:
2024-07-23 13:40:29 metrics/test.rmse:  2.390
2024-07-23 13:40:29 metrics/test.rmse_pcutoff:2.390
2024-07-23 13:40:29 metrics/test.mAP:   79.488
2024-07-23 13:40:29 metrics/test.mAR:   83.333
2024-07-23 13:40:29 metrics/test.mAP_pcutoff:79.488
2024-07-23 13:40:29 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:40:29 Epoch 73/200 (lr=0.0001), train loss 0.00039, valid loss 0.00104
2024-07-23 13:40:58 Training for epoch 74 done, starting evaluation
2024-07-23 13:40:59 Epoch 74 performance:
2024-07-23 13:40:59 metrics/test.rmse:  2.612
2024-07-23 13:40:59 metrics/test.rmse_pcutoff:2.612
2024-07-23 13:40:59 metrics/test.mAP:   70.000
2024-07-23 13:40:59 metrics/test.mAR:   71.667
2024-07-23 13:40:59 metrics/test.mAP_pcutoff:70.000
2024-07-23 13:40:59 metrics/test.mAR_pcutoff:71.667
2024-07-23 13:40:59 Epoch 74/200 (lr=0.0001), train loss 0.00041, valid loss 0.00122
2024-07-23 13:41:27 Training for epoch 75 done, starting evaluation
2024-07-23 13:41:28 Epoch 75 performance:
2024-07-23 13:41:28 metrics/test.rmse:  2.669
2024-07-23 13:41:28 metrics/test.rmse_pcutoff:2.669
2024-07-23 13:41:28 metrics/test.mAP:   74.455
2024-07-23 13:41:28 metrics/test.mAR:   78.333
2024-07-23 13:41:28 metrics/test.mAP_pcutoff:74.455
2024-07-23 13:41:28 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:41:29 Epoch 75/200 (lr=0.0001), train loss 0.00040, valid loss 0.00123
2024-07-23 13:41:59 Training for epoch 76 done, starting evaluation
2024-07-23 13:41:59 Epoch 76 performance:
2024-07-23 13:41:59 metrics/test.rmse:  2.726
2024-07-23 13:41:59 metrics/test.rmse_pcutoff:2.726
2024-07-23 13:41:59 metrics/test.mAP:   65.965
2024-07-23 13:41:59 metrics/test.mAR:   70.000
2024-07-23 13:41:59 metrics/test.mAP_pcutoff:65.965
2024-07-23 13:41:59 metrics/test.mAR_pcutoff:70.000
2024-07-23 13:41:59 Epoch 76/200 (lr=0.0001), train loss 0.00041, valid loss 0.00130
2024-07-23 13:42:29 Training for epoch 77 done, starting evaluation
2024-07-23 13:42:29 Epoch 77 performance:
2024-07-23 13:42:29 metrics/test.rmse:  2.648
2024-07-23 13:42:29 metrics/test.rmse_pcutoff:2.648
2024-07-23 13:42:29 metrics/test.mAP:   73.861
2024-07-23 13:42:29 metrics/test.mAR:   78.333
2024-07-23 13:42:29 metrics/test.mAP_pcutoff:73.861
2024-07-23 13:42:29 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:42:29 Epoch 77/200 (lr=0.0001), train loss 0.00045, valid loss 0.00133
2024-07-23 13:42:58 Training for epoch 78 done, starting evaluation
2024-07-23 13:42:59 Epoch 78 performance:
2024-07-23 13:42:59 metrics/test.rmse:  2.517
2024-07-23 13:42:59 metrics/test.rmse_pcutoff:2.517
2024-07-23 13:42:59 metrics/test.mAP:   78.771
2024-07-23 13:42:59 metrics/test.mAR:   81.667
2024-07-23 13:42:59 metrics/test.mAP_pcutoff:78.771
2024-07-23 13:42:59 metrics/test.mAR_pcutoff:81.667
2024-07-23 13:42:59 Epoch 78/200 (lr=0.0001), train loss 0.00042, valid loss 0.00115
2024-07-23 13:43:28 Training for epoch 79 done, starting evaluation
2024-07-23 13:43:28 Epoch 79 performance:
2024-07-23 13:43:28 metrics/test.rmse:  2.422
2024-07-23 13:43:28 metrics/test.rmse_pcutoff:2.422
2024-07-23 13:43:28 metrics/test.mAP:   76.312
2024-07-23 13:43:28 metrics/test.mAR:   78.333
2024-07-23 13:43:28 metrics/test.mAP_pcutoff:76.312
2024-07-23 13:43:28 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:43:28 Epoch 79/200 (lr=0.0001), train loss 0.00051, valid loss 0.00098
2024-07-23 13:43:55 Training for epoch 80 done, starting evaluation
2024-07-23 13:43:56 Epoch 80 performance:
2024-07-23 13:43:56 metrics/test.rmse:  2.304
2024-07-23 13:43:56 metrics/test.rmse_pcutoff:2.304
2024-07-23 13:43:56 metrics/test.mAP:   82.541
2024-07-23 13:43:56 metrics/test.mAR:   85.000
2024-07-23 13:43:56 metrics/test.mAP_pcutoff:82.541
2024-07-23 13:43:56 metrics/test.mAR_pcutoff:85.000
2024-07-23 13:43:56 Epoch 80/200 (lr=0.0001), train loss 0.00040, valid loss 0.00098
2024-07-23 13:44:25 Training for epoch 81 done, starting evaluation
2024-07-23 13:44:26 Epoch 81 performance:
2024-07-23 13:44:26 metrics/test.rmse:  2.322
2024-07-23 13:44:26 metrics/test.rmse_pcutoff:2.322
2024-07-23 13:44:26 metrics/test.mAP:   80.545
2024-07-23 13:44:26 metrics/test.mAR:   85.000
2024-07-23 13:44:26 metrics/test.mAP_pcutoff:80.545
2024-07-23 13:44:26 metrics/test.mAR_pcutoff:85.000
2024-07-23 13:44:26 Epoch 81/200 (lr=0.0001), train loss 0.00034, valid loss 0.00089
2024-07-23 13:44:56 Training for epoch 82 done, starting evaluation
2024-07-23 13:44:57 Epoch 82 performance:
2024-07-23 13:44:57 metrics/test.rmse:  2.616
2024-07-23 13:44:57 metrics/test.rmse_pcutoff:2.616
2024-07-23 13:44:57 metrics/test.mAP:   71.023
2024-07-23 13:44:57 metrics/test.mAR:   73.333
2024-07-23 13:44:57 metrics/test.mAP_pcutoff:71.023
2024-07-23 13:44:57 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:44:57 Epoch 82/200 (lr=0.0001), train loss 0.00046, valid loss 0.00142
2024-07-23 13:45:26 Training for epoch 83 done, starting evaluation
2024-07-23 13:45:27 Epoch 83 performance:
2024-07-23 13:45:27 metrics/test.rmse:  2.338
2024-07-23 13:45:27 metrics/test.rmse_pcutoff:2.338
2024-07-23 13:45:27 metrics/test.mAP:   76.337
2024-07-23 13:45:27 metrics/test.mAR:   80.000
2024-07-23 13:45:27 metrics/test.mAP_pcutoff:76.337
2024-07-23 13:45:27 metrics/test.mAR_pcutoff:80.000
2024-07-23 13:45:27 Epoch 83/200 (lr=0.0001), train loss 0.00046, valid loss 0.00092
2024-07-23 13:45:56 Training for epoch 84 done, starting evaluation
2024-07-23 13:45:57 Epoch 84 performance:
2024-07-23 13:45:57 metrics/test.rmse:  2.417
2024-07-23 13:45:57 metrics/test.rmse_pcutoff:2.417
2024-07-23 13:45:57 metrics/test.mAP:   80.698
2024-07-23 13:45:57 metrics/test.mAR:   83.333
2024-07-23 13:45:57 metrics/test.mAP_pcutoff:80.698
2024-07-23 13:45:57 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:45:57 Epoch 84/200 (lr=0.0001), train loss 0.00046, valid loss 0.00093
2024-07-23 13:46:27 Training for epoch 85 done, starting evaluation
2024-07-23 13:46:27 Epoch 85 performance:
2024-07-23 13:46:27 metrics/test.rmse:  2.077
2024-07-23 13:46:27 metrics/test.rmse_pcutoff:2.077
2024-07-23 13:46:27 metrics/test.mAP:   84.990
2024-07-23 13:46:27 metrics/test.mAR:   88.333
2024-07-23 13:46:27 metrics/test.mAP_pcutoff:84.990
2024-07-23 13:46:27 metrics/test.mAR_pcutoff:88.333
2024-07-23 13:46:27 Epoch 85/200 (lr=0.0001), train loss 0.00036, valid loss 0.00080
2024-07-23 13:46:56 Training for epoch 86 done, starting evaluation
2024-07-23 13:46:57 Epoch 86 performance:
2024-07-23 13:46:57 metrics/test.rmse:  2.035
2024-07-23 13:46:57 metrics/test.rmse_pcutoff:2.035
2024-07-23 13:46:57 metrics/test.mAP:   81.218
2024-07-23 13:46:57 metrics/test.mAR:   83.333
2024-07-23 13:46:57 metrics/test.mAP_pcutoff:81.218
2024-07-23 13:46:57 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:46:57 Epoch 86/200 (lr=0.0001), train loss 0.00033, valid loss 0.00091
2024-07-23 13:47:25 Training for epoch 87 done, starting evaluation
2024-07-23 13:47:26 Epoch 87 performance:
2024-07-23 13:47:26 metrics/test.rmse:  2.369
2024-07-23 13:47:26 metrics/test.rmse_pcutoff:2.369
2024-07-23 13:47:26 metrics/test.mAP:   79.114
2024-07-23 13:47:26 metrics/test.mAR:   81.667
2024-07-23 13:47:26 metrics/test.mAP_pcutoff:79.114
2024-07-23 13:47:26 metrics/test.mAR_pcutoff:81.667
2024-07-23 13:47:26 Epoch 87/200 (lr=0.0001), train loss 0.00045, valid loss 0.00093
2024-07-23 13:47:56 Training for epoch 88 done, starting evaluation
2024-07-23 13:47:57 Epoch 88 performance:
2024-07-23 13:47:57 metrics/test.rmse:  2.509
2024-07-23 13:47:57 metrics/test.rmse_pcutoff:2.509
2024-07-23 13:47:57 metrics/test.mAP:   76.749
2024-07-23 13:47:57 metrics/test.mAR:   78.333
2024-07-23 13:47:57 metrics/test.mAP_pcutoff:76.749
2024-07-23 13:47:57 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:47:57 Epoch 88/200 (lr=0.0001), train loss 0.00040, valid loss 0.00108
2024-07-23 13:48:26 Training for epoch 89 done, starting evaluation
2024-07-23 13:48:27 Epoch 89 performance:
2024-07-23 13:48:27 metrics/test.rmse:  2.542
2024-07-23 13:48:27 metrics/test.rmse_pcutoff:2.542
2024-07-23 13:48:27 metrics/test.mAP:   72.104
2024-07-23 13:48:27 metrics/test.mAR:   73.333
2024-07-23 13:48:27 metrics/test.mAP_pcutoff:72.104
2024-07-23 13:48:27 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:48:27 Epoch 89/200 (lr=0.0001), train loss 0.00040, valid loss 0.00104
2024-07-23 13:48:54 Training for epoch 90 done, starting evaluation
2024-07-23 13:48:55 Epoch 90 performance:
2024-07-23 13:48:55 metrics/test.rmse:  2.376
2024-07-23 13:48:55 metrics/test.rmse_pcutoff:2.376
2024-07-23 13:48:55 metrics/test.mAP:   79.784
2024-07-23 13:48:55 metrics/test.mAR:   81.667
2024-07-23 13:48:55 metrics/test.mAP_pcutoff:79.784
2024-07-23 13:48:55 metrics/test.mAR_pcutoff:81.667
2024-07-23 13:48:55 Epoch 90/200 (lr=0.0001), train loss 0.00040, valid loss 0.00095
2024-07-23 13:49:24 Training for epoch 91 done, starting evaluation
2024-07-23 13:49:25 Epoch 91 performance:
2024-07-23 13:49:25 metrics/test.rmse:  2.979
2024-07-23 13:49:25 metrics/test.rmse_pcutoff:2.889
2024-07-23 13:49:25 metrics/test.mAP:   62.525
2024-07-23 13:49:25 metrics/test.mAR:   65.000
2024-07-23 13:49:25 metrics/test.mAP_pcutoff:50.000
2024-07-23 13:49:25 metrics/test.mAR_pcutoff:53.333
2024-07-23 13:49:25 Epoch 91/200 (lr=0.0001), train loss 0.00036, valid loss 0.00159
2024-07-23 13:49:55 Training for epoch 92 done, starting evaluation
2024-07-23 13:49:56 Epoch 92 performance:
2024-07-23 13:49:56 metrics/test.rmse:  4.273
2024-07-23 13:49:56 metrics/test.rmse_pcutoff:3.175
2024-07-23 13:49:56 metrics/test.mAP:   41.964
2024-07-23 13:49:56 metrics/test.mAR:   50.000
2024-07-23 13:49:56 metrics/test.mAP_pcutoff:43.465
2024-07-23 13:49:56 metrics/test.mAR_pcutoff:45.000
2024-07-23 13:49:56 Epoch 92/200 (lr=0.0001), train loss 0.00073, valid loss 0.00246
2024-07-23 13:50:25 Training for epoch 93 done, starting evaluation
2024-07-23 13:50:26 Epoch 93 performance:
2024-07-23 13:50:26 metrics/test.rmse:  2.585
2024-07-23 13:50:26 metrics/test.rmse_pcutoff:2.585
2024-07-23 13:50:26 metrics/test.mAP:   72.847
2024-07-23 13:50:26 metrics/test.mAR:   75.000
2024-07-23 13:50:26 metrics/test.mAP_pcutoff:72.847
2024-07-23 13:50:26 metrics/test.mAR_pcutoff:75.000
2024-07-23 13:50:26 Epoch 93/200 (lr=0.0001), train loss 0.00045, valid loss 0.00132
2024-07-23 13:50:55 Training for epoch 94 done, starting evaluation
2024-07-23 13:50:56 Epoch 94 performance:
2024-07-23 13:50:56 metrics/test.rmse:  2.305
2024-07-23 13:50:56 metrics/test.rmse_pcutoff:2.305
2024-07-23 13:50:56 metrics/test.mAP:   82.104
2024-07-23 13:50:56 metrics/test.mAR:   83.333
2024-07-23 13:50:56 metrics/test.mAP_pcutoff:82.104
2024-07-23 13:50:56 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:50:56 Epoch 94/200 (lr=0.0001), train loss 0.00038, valid loss 0.00100
2024-07-23 13:51:24 Training for epoch 95 done, starting evaluation
2024-07-23 13:51:25 Epoch 95 performance:
2024-07-23 13:51:25 metrics/test.rmse:  2.685
2024-07-23 13:51:25 metrics/test.rmse_pcutoff:2.685
2024-07-23 13:51:25 metrics/test.mAP:   77.294
2024-07-23 13:51:25 metrics/test.mAR:   78.333
2024-07-23 13:51:25 metrics/test.mAP_pcutoff:77.294
2024-07-23 13:51:25 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:51:25 Epoch 95/200 (lr=0.0001), train loss 0.00046, valid loss 0.00109
2024-07-23 13:51:54 Training for epoch 96 done, starting evaluation
2024-07-23 13:51:55 Epoch 96 performance:
2024-07-23 13:51:55 metrics/test.rmse:  2.585
2024-07-23 13:51:55 metrics/test.rmse_pcutoff:2.585
2024-07-23 13:51:55 metrics/test.mAP:   72.880
2024-07-23 13:51:55 metrics/test.mAR:   76.667
2024-07-23 13:51:55 metrics/test.mAP_pcutoff:72.880
2024-07-23 13:51:55 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:51:55 Epoch 96/200 (lr=0.0001), train loss 0.00038, valid loss 0.00128
2024-07-23 13:52:24 Training for epoch 97 done, starting evaluation
2024-07-23 13:52:25 Epoch 97 performance:
2024-07-23 13:52:25 metrics/test.rmse:  2.303
2024-07-23 13:52:25 metrics/test.rmse_pcutoff:2.303
2024-07-23 13:52:25 metrics/test.mAP:   80.941
2024-07-23 13:52:25 metrics/test.mAR:   81.667
2024-07-23 13:52:25 metrics/test.mAP_pcutoff:80.941
2024-07-23 13:52:25 metrics/test.mAR_pcutoff:81.667
2024-07-23 13:52:25 Epoch 97/200 (lr=0.0001), train loss 0.00029, valid loss 0.00091
2024-07-23 13:52:56 Training for epoch 98 done, starting evaluation
2024-07-23 13:52:57 Epoch 98 performance:
2024-07-23 13:52:57 metrics/test.rmse:  2.278
2024-07-23 13:52:57 metrics/test.rmse_pcutoff:2.278
2024-07-23 13:52:57 metrics/test.mAP:   81.030
2024-07-23 13:52:57 metrics/test.mAR:   83.333
2024-07-23 13:52:57 metrics/test.mAP_pcutoff:81.030
2024-07-23 13:52:57 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:52:57 Epoch 98/200 (lr=0.0001), train loss 0.00033, valid loss 0.00097
2024-07-23 13:53:26 Training for epoch 99 done, starting evaluation
2024-07-23 13:53:26 Epoch 99 performance:
2024-07-23 13:53:26 metrics/test.rmse:  2.602
2024-07-23 13:53:26 metrics/test.rmse_pcutoff:2.602
2024-07-23 13:53:26 metrics/test.mAP:   75.479
2024-07-23 13:53:26 metrics/test.mAR:   78.333
2024-07-23 13:53:26 metrics/test.mAP_pcutoff:75.479
2024-07-23 13:53:26 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:53:26 Epoch 99/200 (lr=0.0001), train loss 0.00033, valid loss 0.00128
2024-07-23 13:53:56 Training for epoch 100 done, starting evaluation
2024-07-23 13:53:57 Epoch 100 performance:
2024-07-23 13:53:57 metrics/test.rmse:  2.643
2024-07-23 13:53:57 metrics/test.rmse_pcutoff:2.643
2024-07-23 13:53:57 metrics/test.mAP:   72.071
2024-07-23 13:53:57 metrics/test.mAR:   75.000
2024-07-23 13:53:57 metrics/test.mAP_pcutoff:72.071
2024-07-23 13:53:57 metrics/test.mAR_pcutoff:75.000
2024-07-23 13:53:58 Epoch 100/200 (lr=0.0001), train loss 0.00031, valid loss 0.00119
2024-07-23 13:54:27 Training for epoch 101 done, starting evaluation
2024-07-23 13:54:28 Epoch 101 performance:
2024-07-23 13:54:28 metrics/test.rmse:  2.514
2024-07-23 13:54:28 metrics/test.rmse_pcutoff:2.514
2024-07-23 13:54:28 metrics/test.mAP:   73.086
2024-07-23 13:54:28 metrics/test.mAR:   76.667
2024-07-23 13:54:28 metrics/test.mAP_pcutoff:73.086
2024-07-23 13:54:28 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:54:28 Epoch 101/200 (lr=0.0001), train loss 0.00033, valid loss 0.00135
2024-07-23 13:54:57 Training for epoch 102 done, starting evaluation
2024-07-23 13:54:57 Epoch 102 performance:
2024-07-23 13:54:57 metrics/test.rmse:  2.049
2024-07-23 13:54:57 metrics/test.rmse_pcutoff:2.049
2024-07-23 13:54:57 metrics/test.mAP:   82.979
2024-07-23 13:54:57 metrics/test.mAR:   85.000
2024-07-23 13:54:57 metrics/test.mAP_pcutoff:82.979
2024-07-23 13:54:57 metrics/test.mAR_pcutoff:85.000
2024-07-23 13:54:57 Epoch 102/200 (lr=0.0001), train loss 0.00033, valid loss 0.00086
2024-07-23 13:55:26 Training for epoch 103 done, starting evaluation
2024-07-23 13:55:27 Epoch 103 performance:
2024-07-23 13:55:27 metrics/test.rmse:  2.171
2024-07-23 13:55:27 metrics/test.rmse_pcutoff:2.171
2024-07-23 13:55:27 metrics/test.mAP:   81.155
2024-07-23 13:55:27 metrics/test.mAR:   83.333
2024-07-23 13:55:27 metrics/test.mAP_pcutoff:81.155
2024-07-23 13:55:27 metrics/test.mAR_pcutoff:83.333
2024-07-23 13:55:27 Epoch 103/200 (lr=0.0001), train loss 0.00029, valid loss 0.00088
2024-07-23 13:55:56 Training for epoch 104 done, starting evaluation
2024-07-23 13:55:57 Epoch 104 performance:
2024-07-23 13:55:57 metrics/test.rmse:  2.364
2024-07-23 13:55:57 metrics/test.rmse_pcutoff:2.364
2024-07-23 13:55:57 metrics/test.mAP:   73.079
2024-07-23 13:55:57 metrics/test.mAR:   76.667
2024-07-23 13:55:57 metrics/test.mAP_pcutoff:73.079
2024-07-23 13:55:57 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:55:57 Epoch 104/200 (lr=0.0001), train loss 0.00035, valid loss 0.00098
2024-07-23 13:56:29 Training for epoch 105 done, starting evaluation
2024-07-23 13:56:30 Epoch 105 performance:
2024-07-23 13:56:30 metrics/test.rmse:  2.274
2024-07-23 13:56:30 metrics/test.rmse_pcutoff:2.274
2024-07-23 13:56:30 metrics/test.mAP:   76.370
2024-07-23 13:56:30 metrics/test.mAR:   81.667
2024-07-23 13:56:30 metrics/test.mAP_pcutoff:76.370
2024-07-23 13:56:30 metrics/test.mAR_pcutoff:81.667
2024-07-23 13:56:30 Epoch 105/200 (lr=0.0001), train loss 0.00029, valid loss 0.00110
2024-07-23 13:57:03 Training for epoch 106 done, starting evaluation
2024-07-23 13:57:03 Epoch 106 performance:
2024-07-23 13:57:03 metrics/test.rmse:  2.385
2024-07-23 13:57:03 metrics/test.rmse_pcutoff:2.385
2024-07-23 13:57:03 metrics/test.mAP:   76.733
2024-07-23 13:57:03 metrics/test.mAR:   78.333
2024-07-23 13:57:03 metrics/test.mAP_pcutoff:76.733
2024-07-23 13:57:03 metrics/test.mAR_pcutoff:78.333
2024-07-23 13:57:03 Epoch 106/200 (lr=0.0001), train loss 0.00027, valid loss 0.00099
2024-07-23 13:57:32 Training for epoch 107 done, starting evaluation
2024-07-23 13:57:33 Epoch 107 performance:
2024-07-23 13:57:33 metrics/test.rmse:  2.531
2024-07-23 13:57:33 metrics/test.rmse_pcutoff:2.531
2024-07-23 13:57:33 metrics/test.mAP:   74.208
2024-07-23 13:57:33 metrics/test.mAR:   76.667
2024-07-23 13:57:33 metrics/test.mAP_pcutoff:74.208
2024-07-23 13:57:33 metrics/test.mAR_pcutoff:76.667
2024-07-23 13:57:33 Epoch 107/200 (lr=0.0001), train loss 0.00028, valid loss 0.00116
2024-07-23 13:58:02 Training for epoch 108 done, starting evaluation
2024-07-23 13:58:02 Epoch 108 performance:
2024-07-23 13:58:02 metrics/test.rmse:  3.635
2024-07-23 13:58:02 metrics/test.rmse_pcutoff:3.635
2024-07-23 13:58:02 metrics/test.mAP:   58.960
2024-07-23 13:58:02 metrics/test.mAR:   61.667
2024-07-23 13:58:02 metrics/test.mAP_pcutoff:58.960
2024-07-23 13:58:02 metrics/test.mAR_pcutoff:61.667
2024-07-23 13:58:02 Epoch 108/200 (lr=0.0001), train loss 0.00048, valid loss 0.00190
2024-07-23 13:58:32 Training for epoch 109 done, starting evaluation
2024-07-23 13:58:32 Epoch 109 performance:
2024-07-23 13:58:32 metrics/test.rmse:  2.688
2024-07-23 13:58:32 metrics/test.rmse_pcutoff:2.688
2024-07-23 13:58:32 metrics/test.mAP:   70.421
2024-07-23 13:58:32 metrics/test.mAR:   73.333
2024-07-23 13:58:32 metrics/test.mAP_pcutoff:70.421
2024-07-23 13:58:32 metrics/test.mAR_pcutoff:73.333
2024-07-23 13:58:32 Epoch 109/200 (lr=0.0001), train loss 0.00039, valid loss 0.00128
2024-07-23 13:59:01 Training for epoch 110 done, starting evaluation
2024-07-23 13:59:02 Epoch 110 performance:
2024-07-23 13:59:02 metrics/test.rmse:  2.183
2024-07-23 13:59:02 metrics/test.rmse_pcutoff:2.183
2024-07-23 13:59:02 metrics/test.mAP:   85.512
2024-07-23 13:59:02 metrics/test.mAR:   86.667
2024-07-23 13:59:02 metrics/test.mAP_pcutoff:85.512
2024-07-23 13:59:02 metrics/test.mAR_pcutoff:86.667
2024-07-23 13:59:02 Epoch 110/200 (lr=0.0001), train loss 0.00033, valid loss 0.00087
2024-07-23 13:59:31 Training for epoch 111 done, starting evaluation
2024-07-23 13:59:32 Epoch 111 performance:
2024-07-23 13:59:32 metrics/test.rmse:  3.082
2024-07-23 13:59:32 metrics/test.rmse_pcutoff:3.082
2024-07-23 13:59:32 metrics/test.mAP:   62.665
2024-07-23 13:59:32 metrics/test.mAR:   65.000
2024-07-23 13:59:32 metrics/test.mAP_pcutoff:62.665
2024-07-23 13:59:32 metrics/test.mAR_pcutoff:65.000
2024-07-23 13:59:32 Epoch 111/200 (lr=0.0001), train loss 0.00032, valid loss 0.00155
2024-07-23 14:00:01 Training for epoch 112 done, starting evaluation
2024-07-23 14:00:02 Epoch 112 performance:
2024-07-23 14:00:02 metrics/test.rmse:  2.554
2024-07-23 14:00:02 metrics/test.rmse_pcutoff:2.554
2024-07-23 14:00:02 metrics/test.mAP:   70.281
2024-07-23 14:00:02 metrics/test.mAR:   73.333
2024-07-23 14:00:02 metrics/test.mAP_pcutoff:70.281
2024-07-23 14:00:02 metrics/test.mAR_pcutoff:73.333
2024-07-23 14:00:02 Epoch 112/200 (lr=0.0001), train loss 0.00028, valid loss 0.00117
2024-07-23 14:00:31 Training for epoch 113 done, starting evaluation
2024-07-23 14:00:32 Epoch 113 performance:
2024-07-23 14:00:32 metrics/test.rmse:  2.406
2024-07-23 14:00:32 metrics/test.rmse_pcutoff:2.345
2024-07-23 14:00:32 metrics/test.mAP:   75.891
2024-07-23 14:00:32 metrics/test.mAR:   76.667
2024-07-23 14:00:32 metrics/test.mAP_pcutoff:70.842
2024-07-23 14:00:32 metrics/test.mAR_pcutoff:71.667
2024-07-23 14:00:32 Epoch 113/200 (lr=0.0001), train loss 0.00039, valid loss 0.00115
2024-07-23 14:01:02 Training for epoch 114 done, starting evaluation
2024-07-23 14:01:03 Epoch 114 performance:
2024-07-23 14:01:03 metrics/test.rmse:  2.965
2024-07-23 14:01:03 metrics/test.rmse_pcutoff:2.965
2024-07-23 14:01:03 metrics/test.mAP:   66.073
2024-07-23 14:01:03 metrics/test.mAR:   68.333
2024-07-23 14:01:03 metrics/test.mAP_pcutoff:66.073
2024-07-23 14:01:03 metrics/test.mAR_pcutoff:68.333
2024-07-23 14:01:03 Epoch 114/200 (lr=0.0001), train loss 0.00069, valid loss 0.00149
2024-07-23 14:01:32 Training for epoch 115 done, starting evaluation
2024-07-23 14:01:33 Epoch 115 performance:
2024-07-23 14:01:33 metrics/test.rmse:  2.334
2024-07-23 14:01:33 metrics/test.rmse_pcutoff:2.429
2024-07-23 14:01:33 metrics/test.mAP:   71.540
2024-07-23 14:01:33 metrics/test.mAR:   73.333
2024-07-23 14:01:33 metrics/test.mAP_pcutoff:60.281
2024-07-23 14:01:33 metrics/test.mAR_pcutoff:63.333
2024-07-23 14:01:33 Epoch 115/200 (lr=0.0001), train loss 0.00124, valid loss 0.00137
2024-07-23 14:02:03 Training for epoch 116 done, starting evaluation
2024-07-23 14:02:03 Epoch 116 performance:
2024-07-23 14:02:03 metrics/test.rmse:  2.735
2024-07-23 14:02:03 metrics/test.rmse_pcutoff:2.735
2024-07-23 14:02:03 metrics/test.mAP:   73.066
2024-07-23 14:02:03 metrics/test.mAR:   76.667
2024-07-23 14:02:03 metrics/test.mAP_pcutoff:73.066
2024-07-23 14:02:03 metrics/test.mAR_pcutoff:76.667
2024-07-23 14:02:03 Epoch 116/200 (lr=0.0001), train loss 0.00111, valid loss 0.00116
2024-07-23 14:02:32 Training for epoch 117 done, starting evaluation
2024-07-23 14:02:33 Epoch 117 performance:
2024-07-23 14:02:33 metrics/test.rmse:  2.091
2024-07-23 14:02:33 metrics/test.rmse_pcutoff:2.091
2024-07-23 14:02:33 metrics/test.mAP:   84.330
2024-07-23 14:02:33 metrics/test.mAR:   86.667
2024-07-23 14:02:33 metrics/test.mAP_pcutoff:84.330
2024-07-23 14:02:33 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:02:33 Epoch 117/200 (lr=0.0001), train loss 0.00067, valid loss 0.00086
2024-07-23 14:03:02 Training for epoch 118 done, starting evaluation
2024-07-23 14:03:02 Epoch 118 performance:
2024-07-23 14:03:02 metrics/test.rmse:  1.842
2024-07-23 14:03:02 metrics/test.rmse_pcutoff:1.842
2024-07-23 14:03:02 metrics/test.mAP:   84.906
2024-07-23 14:03:02 metrics/test.mAR:   86.667
2024-07-23 14:03:02 metrics/test.mAP_pcutoff:84.906
2024-07-23 14:03:02 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:03:02 Epoch 118/200 (lr=0.0001), train loss 0.00040, valid loss 0.00067
2024-07-23 14:03:31 Training for epoch 119 done, starting evaluation
2024-07-23 14:03:32 Epoch 119 performance:
2024-07-23 14:03:32 metrics/test.rmse:  2.929
2024-07-23 14:03:32 metrics/test.rmse_pcutoff:2.732
2024-07-23 14:03:32 metrics/test.mAP:   67.558
2024-07-23 14:03:32 metrics/test.mAR:   70.000
2024-07-23 14:03:32 metrics/test.mAP_pcutoff:68.960
2024-07-23 14:03:32 metrics/test.mAR_pcutoff:70.000
2024-07-23 14:03:32 Epoch 119/200 (lr=0.0001), train loss 0.00039, valid loss 0.00137
2024-07-23 14:03:57 Training for epoch 120 done, starting evaluation
2024-07-23 14:03:57 Epoch 120 performance:
2024-07-23 14:03:57 metrics/test.rmse:  1.769
2024-07-23 14:03:57 metrics/test.rmse_pcutoff:1.769
2024-07-23 14:03:57 metrics/test.mAP:   90.277
2024-07-23 14:03:57 metrics/test.mAR:   91.667
2024-07-23 14:03:57 metrics/test.mAP_pcutoff:90.277
2024-07-23 14:03:57 metrics/test.mAR_pcutoff:91.667
2024-07-23 14:03:57 Epoch 120/200 (lr=0.0001), train loss 0.00030, valid loss 0.00066
2024-07-23 14:04:16 Training for epoch 121 done, starting evaluation
2024-07-23 14:04:16 Epoch 121 performance:
2024-07-23 14:04:16 metrics/test.rmse:  2.271
2024-07-23 14:04:16 metrics/test.rmse_pcutoff:2.271
2024-07-23 14:04:16 metrics/test.mAP:   73.035
2024-07-23 14:04:16 metrics/test.mAR:   75.000
2024-07-23 14:04:16 metrics/test.mAP_pcutoff:73.035
2024-07-23 14:04:16 metrics/test.mAR_pcutoff:75.000
2024-07-23 14:04:16 Epoch 121/200 (lr=0.0001), train loss 0.00038, valid loss 0.00094
2024-07-23 14:04:35 Training for epoch 122 done, starting evaluation
2024-07-23 14:04:35 Epoch 122 performance:
2024-07-23 14:04:35 metrics/test.rmse:  1.920
2024-07-23 14:04:35 metrics/test.rmse_pcutoff:1.920
2024-07-23 14:04:35 metrics/test.mAP:   84.990
2024-07-23 14:04:35 metrics/test.mAR:   88.333
2024-07-23 14:04:35 metrics/test.mAP_pcutoff:84.990
2024-07-23 14:04:35 metrics/test.mAR_pcutoff:88.333
2024-07-23 14:04:35 Epoch 122/200 (lr=0.0001), train loss 0.00029, valid loss 0.00076
2024-07-23 14:04:54 Training for epoch 123 done, starting evaluation
2024-07-23 14:04:54 Epoch 123 performance:
2024-07-23 14:04:54 metrics/test.rmse:  2.290
2024-07-23 14:04:54 metrics/test.rmse_pcutoff:2.290
2024-07-23 14:04:54 metrics/test.mAP:   79.624
2024-07-23 14:04:54 metrics/test.mAR:   81.667
2024-07-23 14:04:54 metrics/test.mAP_pcutoff:79.624
2024-07-23 14:04:54 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:04:54 Epoch 123/200 (lr=0.0001), train loss 0.00031, valid loss 0.00094
2024-07-23 14:05:12 Training for epoch 124 done, starting evaluation
2024-07-23 14:05:13 Epoch 124 performance:
2024-07-23 14:05:13 metrics/test.rmse:  2.497
2024-07-23 14:05:13 metrics/test.rmse_pcutoff:2.497
2024-07-23 14:05:13 metrics/test.mAP:   77.941
2024-07-23 14:05:13 metrics/test.mAR:   80.000
2024-07-23 14:05:13 metrics/test.mAP_pcutoff:77.941
2024-07-23 14:05:13 metrics/test.mAR_pcutoff:80.000
2024-07-23 14:05:13 Epoch 124/200 (lr=0.0001), train loss 0.00033, valid loss 0.00104
2024-07-23 14:05:30 Training for epoch 125 done, starting evaluation
2024-07-23 14:05:31 Epoch 125 performance:
2024-07-23 14:05:31 metrics/test.rmse:  2.030
2024-07-23 14:05:31 metrics/test.rmse_pcutoff:2.030
2024-07-23 14:05:31 metrics/test.mAP:   86.401
2024-07-23 14:05:31 metrics/test.mAR:   88.333
2024-07-23 14:05:31 metrics/test.mAP_pcutoff:86.401
2024-07-23 14:05:31 metrics/test.mAR_pcutoff:88.333
2024-07-23 14:05:31 Epoch 125/200 (lr=0.0001), train loss 0.00027, valid loss 0.00074
2024-07-23 14:05:48 Training for epoch 126 done, starting evaluation
2024-07-23 14:05:49 Epoch 126 performance:
2024-07-23 14:05:49 metrics/test.rmse:  2.129
2024-07-23 14:05:49 metrics/test.rmse_pcutoff:2.129
2024-07-23 14:05:49 metrics/test.mAP:   82.802
2024-07-23 14:05:49 metrics/test.mAR:   85.000
2024-07-23 14:05:49 metrics/test.mAP_pcutoff:82.802
2024-07-23 14:05:49 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:05:49 Epoch 126/200 (lr=0.0001), train loss 0.00026, valid loss 0.00077
2024-07-23 14:06:06 Training for epoch 127 done, starting evaluation
2024-07-23 14:06:07 Epoch 127 performance:
2024-07-23 14:06:07 metrics/test.rmse:  3.852
2024-07-23 14:06:07 metrics/test.rmse_pcutoff:2.820
2024-07-23 14:06:07 metrics/test.mAP:   53.149
2024-07-23 14:06:07 metrics/test.mAR:   63.333
2024-07-23 14:06:07 metrics/test.mAP_pcutoff:59.340
2024-07-23 14:06:07 metrics/test.mAR_pcutoff:61.667
2024-07-23 14:06:07 Epoch 127/200 (lr=0.0001), train loss 0.00061, valid loss 0.00155
2024-07-23 14:06:24 Training for epoch 128 done, starting evaluation
2024-07-23 14:06:24 Epoch 128 performance:
2024-07-23 14:06:24 metrics/test.rmse:  2.292
2024-07-23 14:06:24 metrics/test.rmse_pcutoff:2.292
2024-07-23 14:06:24 metrics/test.mAP:   78.977
2024-07-23 14:06:24 metrics/test.mAR:   81.667
2024-07-23 14:06:24 metrics/test.mAP_pcutoff:78.977
2024-07-23 14:06:24 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:06:25 Epoch 128/200 (lr=0.0001), train loss 0.00046, valid loss 0.00095
2024-07-23 14:06:42 Training for epoch 129 done, starting evaluation
2024-07-23 14:06:42 Epoch 129 performance:
2024-07-23 14:06:42 metrics/test.rmse:  2.076
2024-07-23 14:06:42 metrics/test.rmse_pcutoff:2.076
2024-07-23 14:06:42 metrics/test.mAP:   84.348
2024-07-23 14:06:42 metrics/test.mAR:   86.667
2024-07-23 14:06:42 metrics/test.mAP_pcutoff:84.348
2024-07-23 14:06:42 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:06:42 Epoch 129/200 (lr=0.0001), train loss 0.00039, valid loss 0.00076
2024-07-23 14:06:59 Training for epoch 130 done, starting evaluation
2024-07-23 14:07:00 Epoch 130 performance:
2024-07-23 14:07:00 metrics/test.rmse:  2.070
2024-07-23 14:07:00 metrics/test.rmse_pcutoff:2.070
2024-07-23 14:07:00 metrics/test.mAP:   82.946
2024-07-23 14:07:00 metrics/test.mAR:   83.333
2024-07-23 14:07:00 metrics/test.mAP_pcutoff:82.946
2024-07-23 14:07:00 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:07:00 Epoch 130/200 (lr=0.0001), train loss 0.00030, valid loss 0.00073
2024-07-23 14:07:17 Training for epoch 131 done, starting evaluation
2024-07-23 14:07:18 Epoch 131 performance:
2024-07-23 14:07:18 metrics/test.rmse:  2.138
2024-07-23 14:07:18 metrics/test.rmse_pcutoff:2.138
2024-07-23 14:07:18 metrics/test.mAP:   78.594
2024-07-23 14:07:18 metrics/test.mAR:   81.667
2024-07-23 14:07:18 metrics/test.mAP_pcutoff:78.594
2024-07-23 14:07:18 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:07:18 Epoch 131/200 (lr=0.0001), train loss 0.00029, valid loss 0.00078
2024-07-23 14:07:35 Training for epoch 132 done, starting evaluation
2024-07-23 14:07:35 Epoch 132 performance:
2024-07-23 14:07:35 metrics/test.rmse:  1.973
2024-07-23 14:07:35 metrics/test.rmse_pcutoff:1.973
2024-07-23 14:07:35 metrics/test.mAP:   84.906
2024-07-23 14:07:35 metrics/test.mAR:   86.667
2024-07-23 14:07:35 metrics/test.mAP_pcutoff:84.906
2024-07-23 14:07:35 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:07:35 Epoch 132/200 (lr=0.0001), train loss 0.00028, valid loss 0.00070
2024-07-23 14:07:53 Training for epoch 133 done, starting evaluation
2024-07-23 14:07:53 Epoch 133 performance:
2024-07-23 14:07:53 metrics/test.rmse:  2.131
2024-07-23 14:07:53 metrics/test.rmse_pcutoff:2.131
2024-07-23 14:07:53 metrics/test.mAP:   82.946
2024-07-23 14:07:53 metrics/test.mAR:   85.000
2024-07-23 14:07:53 metrics/test.mAP_pcutoff:82.946
2024-07-23 14:07:53 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:07:53 Epoch 133/200 (lr=0.0001), train loss 0.00022, valid loss 0.00077
2024-07-23 14:08:15 Training for epoch 134 done, starting evaluation
2024-07-23 14:08:16 Epoch 134 performance:
2024-07-23 14:08:16 metrics/test.rmse:  2.157
2024-07-23 14:08:16 metrics/test.rmse_pcutoff:2.157
2024-07-23 14:08:16 metrics/test.mAP:   79.752
2024-07-23 14:08:16 metrics/test.mAR:   81.667
2024-07-23 14:08:16 metrics/test.mAP_pcutoff:79.752
2024-07-23 14:08:16 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:08:16 Epoch 134/200 (lr=0.0001), train loss 0.00036, valid loss 0.00095
2024-07-23 14:08:45 Training for epoch 135 done, starting evaluation
2024-07-23 14:08:46 Epoch 135 performance:
2024-07-23 14:08:46 metrics/test.rmse:  1.824
2024-07-23 14:08:46 metrics/test.rmse_pcutoff:1.824
2024-07-23 14:08:46 metrics/test.mAP:   86.589
2024-07-23 14:08:46 metrics/test.mAR:   88.333
2024-07-23 14:08:46 metrics/test.mAP_pcutoff:86.589
2024-07-23 14:08:46 metrics/test.mAR_pcutoff:88.333
2024-07-23 14:08:46 Epoch 135/200 (lr=0.0001), train loss 0.00027, valid loss 0.00072
2024-07-23 14:09:14 Training for epoch 136 done, starting evaluation
2024-07-23 14:09:15 Epoch 136 performance:
2024-07-23 14:09:15 metrics/test.rmse:  2.031
2024-07-23 14:09:15 metrics/test.rmse_pcutoff:2.031
2024-07-23 14:09:15 metrics/test.mAP:   85.701
2024-07-23 14:09:15 metrics/test.mAR:   88.333
2024-07-23 14:09:15 metrics/test.mAP_pcutoff:85.701
2024-07-23 14:09:15 metrics/test.mAR_pcutoff:88.333
2024-07-23 14:09:15 Epoch 136/200 (lr=0.0001), train loss 0.00026, valid loss 0.00085
2024-07-23 14:09:44 Training for epoch 137 done, starting evaluation
2024-07-23 14:09:45 Epoch 137 performance:
2024-07-23 14:09:45 metrics/test.rmse:  3.186
2024-07-23 14:09:45 metrics/test.rmse_pcutoff:3.143
2024-07-23 14:09:45 metrics/test.mAP:   61.683
2024-07-23 14:09:45 metrics/test.mAR:   63.333
2024-07-23 14:09:45 metrics/test.mAP_pcutoff:44.271
2024-07-23 14:09:45 metrics/test.mAR_pcutoff:46.667
2024-07-23 14:09:45 Epoch 137/200 (lr=0.0001), train loss 0.00045, valid loss 0.00197
2024-07-23 14:10:15 Training for epoch 138 done, starting evaluation
2024-07-23 14:10:16 Epoch 138 performance:
2024-07-23 14:10:16 metrics/test.rmse:  2.509
2024-07-23 14:10:16 metrics/test.rmse_pcutoff:2.509
2024-07-23 14:10:16 metrics/test.mAP:   77.475
2024-07-23 14:10:16 metrics/test.mAR:   78.333
2024-07-23 14:10:16 metrics/test.mAP_pcutoff:77.475
2024-07-23 14:10:16 metrics/test.mAR_pcutoff:78.333
2024-07-23 14:10:16 Epoch 138/200 (lr=0.0001), train loss 0.00059, valid loss 0.00125
2024-07-23 14:10:45 Training for epoch 139 done, starting evaluation
2024-07-23 14:10:46 Epoch 139 performance:
2024-07-23 14:10:46 metrics/test.rmse:  2.410
2024-07-23 14:10:46 metrics/test.rmse_pcutoff:2.410
2024-07-23 14:10:46 metrics/test.mAP:   72.203
2024-07-23 14:10:46 metrics/test.mAR:   75.000
2024-07-23 14:10:46 metrics/test.mAP_pcutoff:72.203
2024-07-23 14:10:46 metrics/test.mAR_pcutoff:75.000
2024-07-23 14:10:46 Epoch 139/200 (lr=0.0001), train loss 0.00040, valid loss 0.00106
2024-07-23 14:11:15 Training for epoch 140 done, starting evaluation
2024-07-23 14:11:15 Epoch 140 performance:
2024-07-23 14:11:15 metrics/test.rmse:  1.978
2024-07-23 14:11:15 metrics/test.rmse_pcutoff:1.978
2024-07-23 14:11:15 metrics/test.mAP:   85.980
2024-07-23 14:11:15 metrics/test.mAR:   88.333
2024-07-23 14:11:15 metrics/test.mAP_pcutoff:85.980
2024-07-23 14:11:15 metrics/test.mAR_pcutoff:88.333
2024-07-23 14:11:15 Epoch 140/200 (lr=0.0001), train loss 0.00029, valid loss 0.00065
2024-07-23 14:11:45 Training for epoch 141 done, starting evaluation
2024-07-23 14:11:46 Epoch 141 performance:
2024-07-23 14:11:46 metrics/test.rmse:  2.471
2024-07-23 14:11:46 metrics/test.rmse_pcutoff:2.471
2024-07-23 14:11:46 metrics/test.mAP:   71.960
2024-07-23 14:11:46 metrics/test.mAR:   75.000
2024-07-23 14:11:46 metrics/test.mAP_pcutoff:71.960
2024-07-23 14:11:46 metrics/test.mAR_pcutoff:75.000
2024-07-23 14:11:46 Epoch 141/200 (lr=0.0001), train loss 0.00027, valid loss 0.00099
2024-07-23 14:12:15 Training for epoch 142 done, starting evaluation
2024-07-23 14:12:16 Epoch 142 performance:
2024-07-23 14:12:16 metrics/test.rmse:  2.422
2024-07-23 14:12:16 metrics/test.rmse_pcutoff:2.422
2024-07-23 14:12:16 metrics/test.mAP:   73.647
2024-07-23 14:12:16 metrics/test.mAR:   76.667
2024-07-23 14:12:16 metrics/test.mAP_pcutoff:73.647
2024-07-23 14:12:16 metrics/test.mAR_pcutoff:76.667
2024-07-23 14:12:16 Epoch 142/200 (lr=0.0001), train loss 0.00025, valid loss 0.00101
2024-07-23 14:12:45 Training for epoch 143 done, starting evaluation
2024-07-23 14:12:46 Epoch 143 performance:
2024-07-23 14:12:46 metrics/test.rmse:  2.429
2024-07-23 14:12:46 metrics/test.rmse_pcutoff:2.429
2024-07-23 14:12:46 metrics/test.mAP:   73.787
2024-07-23 14:12:46 metrics/test.mAR:   76.667
2024-07-23 14:12:46 metrics/test.mAP_pcutoff:73.787
2024-07-23 14:12:46 metrics/test.mAR_pcutoff:76.667
2024-07-23 14:12:46 Epoch 143/200 (lr=0.0001), train loss 0.00028, valid loss 0.00112
2024-07-23 14:13:15 Training for epoch 144 done, starting evaluation
2024-07-23 14:13:16 Epoch 144 performance:
2024-07-23 14:13:16 metrics/test.rmse:  2.630
2024-07-23 14:13:16 metrics/test.rmse_pcutoff:2.630
2024-07-23 14:13:16 metrics/test.mAP:   69.761
2024-07-23 14:13:16 metrics/test.mAR:   73.333
2024-07-23 14:13:16 metrics/test.mAP_pcutoff:69.761
2024-07-23 14:13:16 metrics/test.mAR_pcutoff:73.333
2024-07-23 14:13:16 Epoch 144/200 (lr=0.0001), train loss 0.00026, valid loss 0.00135
2024-07-23 14:13:46 Training for epoch 145 done, starting evaluation
2024-07-23 14:13:46 Epoch 145 performance:
2024-07-23 14:13:46 metrics/test.rmse:  1.935
2024-07-23 14:13:46 metrics/test.rmse_pcutoff:1.935
2024-07-23 14:13:46 metrics/test.mAP:   85.139
2024-07-23 14:13:46 metrics/test.mAR:   86.667
2024-07-23 14:13:46 metrics/test.mAP_pcutoff:85.139
2024-07-23 14:13:46 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:13:46 Epoch 145/200 (lr=0.0001), train loss 0.00028, valid loss 0.00068
2024-07-23 14:14:16 Training for epoch 146 done, starting evaluation
2024-07-23 14:14:16 Epoch 146 performance:
2024-07-23 14:14:16 metrics/test.rmse:  1.951
2024-07-23 14:14:16 metrics/test.rmse_pcutoff:1.951
2024-07-23 14:14:16 metrics/test.mAP:   85.487
2024-07-23 14:14:16 metrics/test.mAR:   86.667
2024-07-23 14:14:16 metrics/test.mAP_pcutoff:85.487
2024-07-23 14:14:16 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:14:16 Epoch 146/200 (lr=0.0001), train loss 0.00025, valid loss 0.00076
2024-07-23 14:14:46 Training for epoch 147 done, starting evaluation
2024-07-23 14:14:47 Epoch 147 performance:
2024-07-23 14:14:47 metrics/test.rmse:  2.139
2024-07-23 14:14:47 metrics/test.rmse_pcutoff:2.139
2024-07-23 14:14:47 metrics/test.mAP:   84.208
2024-07-23 14:14:47 metrics/test.mAR:   85.000
2024-07-23 14:14:47 metrics/test.mAP_pcutoff:84.208
2024-07-23 14:14:47 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:14:47 Epoch 147/200 (lr=0.0001), train loss 0.00025, valid loss 0.00081
2024-07-23 14:15:15 Training for epoch 148 done, starting evaluation
2024-07-23 14:15:15 Epoch 148 performance:
2024-07-23 14:15:15 metrics/test.rmse:  3.486
2024-07-23 14:15:15 metrics/test.rmse_pcutoff:3.486
2024-07-23 14:15:15 metrics/test.mAP:   66.955
2024-07-23 14:15:15 metrics/test.mAR:   70.000
2024-07-23 14:15:15 metrics/test.mAP_pcutoff:66.955
2024-07-23 14:15:15 metrics/test.mAR_pcutoff:70.000
2024-07-23 14:15:15 Epoch 148/200 (lr=0.0001), train loss 0.00023, valid loss 0.00167
2024-07-23 14:15:34 Training for epoch 149 done, starting evaluation
2024-07-23 14:15:34 Epoch 149 performance:
2024-07-23 14:15:34 metrics/test.rmse:  2.399
2024-07-23 14:15:34 metrics/test.rmse_pcutoff:2.399
2024-07-23 14:15:34 metrics/test.mAP:   78.177
2024-07-23 14:15:34 metrics/test.mAR:   80.000
2024-07-23 14:15:34 metrics/test.mAP_pcutoff:78.177
2024-07-23 14:15:34 metrics/test.mAR_pcutoff:80.000
2024-07-23 14:15:34 Epoch 149/200 (lr=0.0001), train loss 0.00037, valid loss 0.00115
2024-07-23 14:15:53 Training for epoch 150 done, starting evaluation
2024-07-23 14:15:53 Epoch 150 performance:
2024-07-23 14:15:53 metrics/test.rmse:  1.788
2024-07-23 14:15:53 metrics/test.rmse_pcutoff:1.788
2024-07-23 14:15:53 metrics/test.mAP:   88.505
2024-07-23 14:15:53 metrics/test.mAR:   90.000
2024-07-23 14:15:53 metrics/test.mAP_pcutoff:88.505
2024-07-23 14:15:53 metrics/test.mAR_pcutoff:90.000
2024-07-23 14:15:53 Epoch 150/200 (lr=0.0001), train loss 0.00035, valid loss 0.00071
2024-07-23 14:16:12 Training for epoch 151 done, starting evaluation
2024-07-23 14:16:12 Epoch 151 performance:
2024-07-23 14:16:12 metrics/test.rmse:  2.163
2024-07-23 14:16:12 metrics/test.rmse_pcutoff:2.163
2024-07-23 14:16:12 metrics/test.mAP:   82.292
2024-07-23 14:16:12 metrics/test.mAR:   83.333
2024-07-23 14:16:12 metrics/test.mAP_pcutoff:82.292
2024-07-23 14:16:12 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:16:12 Epoch 151/200 (lr=0.0001), train loss 0.00033, valid loss 0.00083
2024-07-23 14:16:32 Training for epoch 152 done, starting evaluation
2024-07-23 14:16:32 Epoch 152 performance:
2024-07-23 14:16:32 metrics/test.rmse:  2.210
2024-07-23 14:16:32 metrics/test.rmse_pcutoff:2.210
2024-07-23 14:16:32 metrics/test.mAP:   82.104
2024-07-23 14:16:32 metrics/test.mAR:   83.333
2024-07-23 14:16:32 metrics/test.mAP_pcutoff:82.104
2024-07-23 14:16:32 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:16:32 Epoch 152/200 (lr=0.0001), train loss 0.00036, valid loss 0.00086
2024-07-23 14:16:51 Training for epoch 153 done, starting evaluation
2024-07-23 14:16:51 Epoch 153 performance:
2024-07-23 14:16:51 metrics/test.rmse:  2.150
2024-07-23 14:16:51 metrics/test.rmse_pcutoff:2.150
2024-07-23 14:16:51 metrics/test.mAP:   83.644
2024-07-23 14:16:51 metrics/test.mAR:   85.000
2024-07-23 14:16:51 metrics/test.mAP_pcutoff:83.644
2024-07-23 14:16:51 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:16:51 Epoch 153/200 (lr=0.0001), train loss 0.00028, valid loss 0.00084
2024-07-23 14:17:15 Training for epoch 154 done, starting evaluation
2024-07-23 14:17:16 Epoch 154 performance:
2024-07-23 14:17:16 metrics/test.rmse:  2.188
2024-07-23 14:17:16 metrics/test.rmse_pcutoff:2.188
2024-07-23 14:17:16 metrics/test.mAP:   78.738
2024-07-23 14:17:16 metrics/test.mAR:   80.000
2024-07-23 14:17:16 metrics/test.mAP_pcutoff:78.738
2024-07-23 14:17:16 metrics/test.mAR_pcutoff:80.000
2024-07-23 14:17:16 Epoch 154/200 (lr=0.0001), train loss 0.00024, valid loss 0.00071
2024-07-23 14:17:44 Training for epoch 155 done, starting evaluation
2024-07-23 14:17:45 Epoch 155 performance:
2024-07-23 14:17:45 metrics/test.rmse:  2.072
2024-07-23 14:17:45 metrics/test.rmse_pcutoff:2.072
2024-07-23 14:17:45 metrics/test.mAP:   87.663
2024-07-23 14:17:45 metrics/test.mAR:   88.333
2024-07-23 14:17:45 metrics/test.mAP_pcutoff:87.663
2024-07-23 14:17:45 metrics/test.mAR_pcutoff:88.333
2024-07-23 14:17:45 Epoch 155/200 (lr=0.0001), train loss 0.00023, valid loss 0.00079
2024-07-23 14:18:12 Training for epoch 156 done, starting evaluation
2024-07-23 14:18:12 Epoch 156 performance:
2024-07-23 14:18:12 metrics/test.rmse:  2.171
2024-07-23 14:18:12 metrics/test.rmse_pcutoff:2.171
2024-07-23 14:18:12 metrics/test.mAP:   79.620
2024-07-23 14:18:12 metrics/test.mAR:   81.667
2024-07-23 14:18:12 metrics/test.mAP_pcutoff:79.620
2024-07-23 14:18:12 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:18:12 Epoch 156/200 (lr=0.0001), train loss 0.00038, valid loss 0.00088
2024-07-23 14:18:36 Training for epoch 157 done, starting evaluation
2024-07-23 14:18:37 Epoch 157 performance:
2024-07-23 14:18:37 metrics/test.rmse:  2.656
2024-07-23 14:18:37 metrics/test.rmse_pcutoff:2.656
2024-07-23 14:18:37 metrics/test.mAP:   70.322
2024-07-23 14:18:37 metrics/test.mAR:   73.333
2024-07-23 14:18:37 metrics/test.mAP_pcutoff:70.322
2024-07-23 14:18:37 metrics/test.mAR_pcutoff:73.333
2024-07-23 14:18:37 Epoch 157/200 (lr=0.0001), train loss 0.00047, valid loss 0.00113
2024-07-23 14:19:05 Training for epoch 158 done, starting evaluation
2024-07-23 14:19:06 Epoch 158 performance:
2024-07-23 14:19:06 metrics/test.rmse:  2.592
2024-07-23 14:19:06 metrics/test.rmse_pcutoff:2.592
2024-07-23 14:19:06 metrics/test.mAP:   68.771
2024-07-23 14:19:06 metrics/test.mAR:   73.333
2024-07-23 14:19:06 metrics/test.mAP_pcutoff:68.771
2024-07-23 14:19:06 metrics/test.mAR_pcutoff:73.333
2024-07-23 14:19:06 Epoch 158/200 (lr=0.0001), train loss 0.00033, valid loss 0.00152
2024-07-23 14:19:35 Training for epoch 159 done, starting evaluation
2024-07-23 14:19:36 Epoch 159 performance:
2024-07-23 14:19:36 metrics/test.rmse:  2.877
2024-07-23 14:19:36 metrics/test.rmse_pcutoff:2.877
2024-07-23 14:19:36 metrics/test.mAP:   66.774
2024-07-23 14:19:36 metrics/test.mAR:   68.333
2024-07-23 14:19:36 metrics/test.mAP_pcutoff:66.774
2024-07-23 14:19:36 metrics/test.mAR_pcutoff:68.333
2024-07-23 14:19:36 Epoch 159/200 (lr=0.0001), train loss 0.00170, valid loss 0.00141
2024-07-23 14:20:06 Training for epoch 160 done, starting evaluation
2024-07-23 14:20:07 Epoch 160 performance:
2024-07-23 14:20:07 metrics/test.rmse:  3.051
2024-07-23 14:20:07 metrics/test.rmse_pcutoff:3.196
2024-07-23 14:20:07 metrics/test.mAP:   60.099
2024-07-23 14:20:07 metrics/test.mAR:   61.667
2024-07-23 14:20:07 metrics/test.mAP_pcutoff:53.564
2024-07-23 14:20:07 metrics/test.mAR_pcutoff:53.333
2024-07-23 14:20:07 Epoch 160/200 (lr=1e-05), train loss 0.00061, valid loss 0.00179
2024-07-23 14:20:36 Training for epoch 161 done, starting evaluation
2024-07-23 14:20:37 Epoch 161 performance:
2024-07-23 14:20:37 metrics/test.rmse:  2.432
2024-07-23 14:20:37 metrics/test.rmse_pcutoff:2.432
2024-07-23 14:20:37 metrics/test.mAP:   73.606
2024-07-23 14:20:37 metrics/test.mAR:   76.667
2024-07-23 14:20:37 metrics/test.mAP_pcutoff:73.606
2024-07-23 14:20:37 metrics/test.mAR_pcutoff:76.667
2024-07-23 14:20:37 Epoch 161/200 (lr=1e-05), train loss 0.00044, valid loss 0.00107
2024-07-23 14:21:06 Training for epoch 162 done, starting evaluation
2024-07-23 14:21:07 Epoch 162 performance:
2024-07-23 14:21:07 metrics/test.rmse:  2.288
2024-07-23 14:21:07 metrics/test.rmse_pcutoff:2.288
2024-07-23 14:21:07 metrics/test.mAP:   79.571
2024-07-23 14:21:07 metrics/test.mAR:   81.667
2024-07-23 14:21:07 metrics/test.mAP_pcutoff:79.571
2024-07-23 14:21:07 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:21:07 Epoch 162/200 (lr=1e-05), train loss 0.00032, valid loss 0.00098
2024-07-23 14:21:36 Training for epoch 163 done, starting evaluation
2024-07-23 14:21:37 Epoch 163 performance:
2024-07-23 14:21:37 metrics/test.rmse:  2.300
2024-07-23 14:21:37 metrics/test.rmse_pcutoff:2.300
2024-07-23 14:21:37 metrics/test.mAP:   76.625
2024-07-23 14:21:37 metrics/test.mAR:   80.000
2024-07-23 14:21:37 metrics/test.mAP_pcutoff:76.625
2024-07-23 14:21:37 metrics/test.mAR_pcutoff:80.000
2024-07-23 14:21:37 Epoch 163/200 (lr=1e-05), train loss 0.00028, valid loss 0.00098
2024-07-23 14:22:06 Training for epoch 164 done, starting evaluation
2024-07-23 14:22:06 Epoch 164 performance:
2024-07-23 14:22:06 metrics/test.rmse:  2.265
2024-07-23 14:22:06 metrics/test.rmse_pcutoff:2.265
2024-07-23 14:22:06 metrics/test.mAP:   76.625
2024-07-23 14:22:06 metrics/test.mAR:   80.000
2024-07-23 14:22:06 metrics/test.mAP_pcutoff:76.625
2024-07-23 14:22:06 metrics/test.mAR_pcutoff:80.000
2024-07-23 14:22:06 Epoch 164/200 (lr=1e-05), train loss 0.00028, valid loss 0.00095
2024-07-23 14:22:36 Training for epoch 165 done, starting evaluation
2024-07-23 14:22:37 Epoch 165 performance:
2024-07-23 14:22:37 metrics/test.rmse:  2.240
2024-07-23 14:22:37 metrics/test.rmse_pcutoff:2.240
2024-07-23 14:22:37 metrics/test.mAP:   79.571
2024-07-23 14:22:37 metrics/test.mAR:   81.667
2024-07-23 14:22:37 metrics/test.mAP_pcutoff:79.571
2024-07-23 14:22:37 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:22:37 Epoch 165/200 (lr=1e-05), train loss 0.00026, valid loss 0.00092
2024-07-23 14:23:06 Training for epoch 166 done, starting evaluation
2024-07-23 14:23:07 Epoch 166 performance:
2024-07-23 14:23:07 metrics/test.rmse:  2.246
2024-07-23 14:23:07 metrics/test.rmse_pcutoff:2.246
2024-07-23 14:23:07 metrics/test.mAP:   78.977
2024-07-23 14:23:07 metrics/test.mAR:   81.667
2024-07-23 14:23:07 metrics/test.mAP_pcutoff:78.977
2024-07-23 14:23:07 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:23:07 Epoch 166/200 (lr=1e-05), train loss 0.00023, valid loss 0.00088
2024-07-23 14:23:33 Training for epoch 167 done, starting evaluation
2024-07-23 14:23:34 Epoch 167 performance:
2024-07-23 14:23:34 metrics/test.rmse:  2.233
2024-07-23 14:23:34 metrics/test.rmse_pcutoff:2.233
2024-07-23 14:23:34 metrics/test.mAP:   81.922
2024-07-23 14:23:34 metrics/test.mAR:   83.333
2024-07-23 14:23:34 metrics/test.mAP_pcutoff:81.922
2024-07-23 14:23:34 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:23:34 Epoch 167/200 (lr=1e-05), train loss 0.00021, valid loss 0.00089
2024-07-23 14:24:03 Training for epoch 168 done, starting evaluation
2024-07-23 14:24:04 Epoch 168 performance:
2024-07-23 14:24:04 metrics/test.rmse:  2.253
2024-07-23 14:24:04 metrics/test.rmse_pcutoff:2.253
2024-07-23 14:24:04 metrics/test.mAP:   81.922
2024-07-23 14:24:04 metrics/test.mAR:   83.333
2024-07-23 14:24:04 metrics/test.mAP_pcutoff:81.922
2024-07-23 14:24:04 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:24:04 Epoch 168/200 (lr=1e-05), train loss 0.00021, valid loss 0.00091
2024-07-23 14:24:34 Training for epoch 169 done, starting evaluation
2024-07-23 14:24:34 Epoch 169 performance:
2024-07-23 14:24:34 metrics/test.rmse:  2.184
2024-07-23 14:24:34 metrics/test.rmse_pcutoff:2.184
2024-07-23 14:24:34 metrics/test.mAP:   82.979
2024-07-23 14:24:34 metrics/test.mAR:   85.000
2024-07-23 14:24:34 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:24:34 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:24:34 Epoch 169/200 (lr=1e-05), train loss 0.00021, valid loss 0.00086
2024-07-23 14:25:04 Training for epoch 170 done, starting evaluation
2024-07-23 14:25:05 Epoch 170 performance:
2024-07-23 14:25:05 metrics/test.rmse:  2.207
2024-07-23 14:25:05 metrics/test.rmse_pcutoff:2.207
2024-07-23 14:25:05 metrics/test.mAP:   83.886
2024-07-23 14:25:05 metrics/test.mAR:   85.000
2024-07-23 14:25:05 metrics/test.mAP_pcutoff:83.886
2024-07-23 14:25:05 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:25:05 Epoch 170/200 (lr=1e-05), train loss 0.00024, valid loss 0.00085
2024-07-23 14:25:34 Training for epoch 171 done, starting evaluation
2024-07-23 14:25:35 Epoch 171 performance:
2024-07-23 14:25:35 metrics/test.rmse:  2.197
2024-07-23 14:25:35 metrics/test.rmse_pcutoff:2.197
2024-07-23 14:25:35 metrics/test.mAP:   82.979
2024-07-23 14:25:35 metrics/test.mAR:   85.000
2024-07-23 14:25:35 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:25:35 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:25:35 Epoch 171/200 (lr=1e-05), train loss 0.00024, valid loss 0.00092
2024-07-23 14:26:04 Training for epoch 172 done, starting evaluation
2024-07-23 14:26:05 Epoch 172 performance:
2024-07-23 14:26:05 metrics/test.rmse:  2.212
2024-07-23 14:26:05 metrics/test.rmse_pcutoff:2.212
2024-07-23 14:26:05 metrics/test.mAP:   82.979
2024-07-23 14:26:05 metrics/test.mAR:   85.000
2024-07-23 14:26:05 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:26:05 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:26:05 Epoch 172/200 (lr=1e-05), train loss 0.00022, valid loss 0.00092
2024-07-23 14:26:34 Training for epoch 173 done, starting evaluation
2024-07-23 14:26:35 Epoch 173 performance:
2024-07-23 14:26:35 metrics/test.rmse:  2.207
2024-07-23 14:26:35 metrics/test.rmse_pcutoff:2.207
2024-07-23 14:26:35 metrics/test.mAP:   82.979
2024-07-23 14:26:35 metrics/test.mAR:   85.000
2024-07-23 14:26:35 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:26:35 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:26:35 Epoch 173/200 (lr=1e-05), train loss 0.00019, valid loss 0.00090
2024-07-23 14:27:05 Training for epoch 174 done, starting evaluation
2024-07-23 14:27:05 Epoch 174 performance:
2024-07-23 14:27:05 metrics/test.rmse:  2.228
2024-07-23 14:27:05 metrics/test.rmse_pcutoff:2.228
2024-07-23 14:27:05 metrics/test.mAP:   82.979
2024-07-23 14:27:05 metrics/test.mAR:   85.000
2024-07-23 14:27:05 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:27:05 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:27:05 Epoch 174/200 (lr=1e-05), train loss 0.00019, valid loss 0.00091
2024-07-23 14:27:35 Training for epoch 175 done, starting evaluation
2024-07-23 14:27:35 Epoch 175 performance:
2024-07-23 14:27:35 metrics/test.rmse:  2.140
2024-07-23 14:27:35 metrics/test.rmse_pcutoff:2.140
2024-07-23 14:27:35 metrics/test.mAP:   82.979
2024-07-23 14:27:35 metrics/test.mAR:   85.000
2024-07-23 14:27:35 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:27:35 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:27:36 Epoch 175/200 (lr=1e-05), train loss 0.00020, valid loss 0.00088
2024-07-23 14:28:06 Training for epoch 176 done, starting evaluation
2024-07-23 14:28:07 Epoch 176 performance:
2024-07-23 14:28:07 metrics/test.rmse:  2.112
2024-07-23 14:28:07 metrics/test.rmse_pcutoff:2.112
2024-07-23 14:28:07 metrics/test.mAP:   84.942
2024-07-23 14:28:07 metrics/test.mAR:   86.667
2024-07-23 14:28:07 metrics/test.mAP_pcutoff:84.942
2024-07-23 14:28:07 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:28:07 Epoch 176/200 (lr=1e-05), train loss 0.00020, valid loss 0.00088
2024-07-23 14:28:36 Training for epoch 177 done, starting evaluation
2024-07-23 14:28:37 Epoch 177 performance:
2024-07-23 14:28:37 metrics/test.rmse:  2.135
2024-07-23 14:28:37 metrics/test.rmse_pcutoff:2.135
2024-07-23 14:28:37 metrics/test.mAP:   84.942
2024-07-23 14:28:37 metrics/test.mAR:   86.667
2024-07-23 14:28:37 metrics/test.mAP_pcutoff:84.942
2024-07-23 14:28:37 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:28:37 Epoch 177/200 (lr=1e-05), train loss 0.00019, valid loss 0.00086
2024-07-23 14:29:07 Training for epoch 178 done, starting evaluation
2024-07-23 14:29:07 Epoch 178 performance:
2024-07-23 14:29:07 metrics/test.rmse:  2.107
2024-07-23 14:29:07 metrics/test.rmse_pcutoff:2.107
2024-07-23 14:29:07 metrics/test.mAP:   84.035
2024-07-23 14:29:07 metrics/test.mAR:   86.667
2024-07-23 14:29:07 metrics/test.mAP_pcutoff:84.035
2024-07-23 14:29:07 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:29:07 Epoch 178/200 (lr=1e-05), train loss 0.00020, valid loss 0.00087
2024-07-23 14:29:36 Training for epoch 179 done, starting evaluation
2024-07-23 14:29:37 Epoch 179 performance:
2024-07-23 14:29:37 metrics/test.rmse:  2.113
2024-07-23 14:29:37 metrics/test.rmse_pcutoff:2.113
2024-07-23 14:29:37 metrics/test.mAP:   81.931
2024-07-23 14:29:37 metrics/test.mAR:   85.000
2024-07-23 14:29:37 metrics/test.mAP_pcutoff:81.931
2024-07-23 14:29:37 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:29:37 Epoch 179/200 (lr=1e-05), train loss 0.00018, valid loss 0.00087
2024-07-23 14:30:06 Training for epoch 180 done, starting evaluation
2024-07-23 14:30:06 Epoch 180 performance:
2024-07-23 14:30:06 metrics/test.rmse:  2.188
2024-07-23 14:30:06 metrics/test.rmse_pcutoff:2.188
2024-07-23 14:30:06 metrics/test.mAP:   80.875
2024-07-23 14:30:06 metrics/test.mAR:   83.333
2024-07-23 14:30:06 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:30:06 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:30:06 Epoch 180/200 (lr=1e-05), train loss 0.00017, valid loss 0.00091
2024-07-23 14:30:36 Training for epoch 181 done, starting evaluation
2024-07-23 14:30:36 Epoch 181 performance:
2024-07-23 14:30:36 metrics/test.rmse:  2.098
2024-07-23 14:30:36 metrics/test.rmse_pcutoff:2.098
2024-07-23 14:30:36 metrics/test.mAP:   84.035
2024-07-23 14:30:36 metrics/test.mAR:   86.667
2024-07-23 14:30:36 metrics/test.mAP_pcutoff:84.035
2024-07-23 14:30:36 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:30:37 Epoch 181/200 (lr=1e-05), train loss 0.00022, valid loss 0.00085
2024-07-23 14:31:05 Training for epoch 182 done, starting evaluation
2024-07-23 14:31:06 Epoch 182 performance:
2024-07-23 14:31:06 metrics/test.rmse:  2.175
2024-07-23 14:31:06 metrics/test.rmse_pcutoff:2.175
2024-07-23 14:31:06 metrics/test.mAP:   82.979
2024-07-23 14:31:06 metrics/test.mAR:   85.000
2024-07-23 14:31:06 metrics/test.mAP_pcutoff:82.979
2024-07-23 14:31:06 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:31:06 Epoch 182/200 (lr=1e-05), train loss 0.00020, valid loss 0.00088
2024-07-23 14:31:35 Training for epoch 183 done, starting evaluation
2024-07-23 14:31:36 Epoch 183 performance:
2024-07-23 14:31:36 metrics/test.rmse:  2.098
2024-07-23 14:31:36 metrics/test.rmse_pcutoff:2.098
2024-07-23 14:31:36 metrics/test.mAP:   84.035
2024-07-23 14:31:36 metrics/test.mAR:   86.667
2024-07-23 14:31:36 metrics/test.mAP_pcutoff:84.035
2024-07-23 14:31:36 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:31:36 Epoch 183/200 (lr=1e-05), train loss 0.00018, valid loss 0.00086
2024-07-23 14:32:04 Training for epoch 184 done, starting evaluation
2024-07-23 14:32:04 Epoch 184 performance:
2024-07-23 14:32:04 metrics/test.rmse:  2.096
2024-07-23 14:32:04 metrics/test.rmse_pcutoff:2.096
2024-07-23 14:32:04 metrics/test.mAP:   84.035
2024-07-23 14:32:04 metrics/test.mAR:   86.667
2024-07-23 14:32:04 metrics/test.mAP_pcutoff:84.035
2024-07-23 14:32:04 metrics/test.mAR_pcutoff:86.667
2024-07-23 14:32:04 Epoch 184/200 (lr=1e-05), train loss 0.00021, valid loss 0.00088
2024-07-23 14:32:23 Training for epoch 185 done, starting evaluation
2024-07-23 14:32:23 Epoch 185 performance:
2024-07-23 14:32:23 metrics/test.rmse:  2.130
2024-07-23 14:32:23 metrics/test.rmse_pcutoff:2.130
2024-07-23 14:32:23 metrics/test.mAP:   81.015
2024-07-23 14:32:23 metrics/test.mAR:   83.333
2024-07-23 14:32:23 metrics/test.mAP_pcutoff:81.015
2024-07-23 14:32:23 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:32:23 Epoch 185/200 (lr=1e-05), train loss 0.00017, valid loss 0.00089
2024-07-23 14:32:42 Training for epoch 186 done, starting evaluation
2024-07-23 14:32:42 Epoch 186 performance:
2024-07-23 14:32:42 metrics/test.rmse:  2.200
2024-07-23 14:32:42 metrics/test.rmse_pcutoff:2.200
2024-07-23 14:32:42 metrics/test.mAP:   79.959
2024-07-23 14:32:42 metrics/test.mAR:   81.667
2024-07-23 14:32:42 metrics/test.mAP_pcutoff:79.959
2024-07-23 14:32:42 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:32:42 Epoch 186/200 (lr=1e-05), train loss 0.00017, valid loss 0.00092
2024-07-23 14:33:02 Training for epoch 187 done, starting evaluation
2024-07-23 14:33:02 Epoch 187 performance:
2024-07-23 14:33:02 metrics/test.rmse:  2.147
2024-07-23 14:33:02 metrics/test.rmse_pcutoff:2.147
2024-07-23 14:33:02 metrics/test.mAP:   79.959
2024-07-23 14:33:02 metrics/test.mAR:   81.667
2024-07-23 14:33:02 metrics/test.mAP_pcutoff:79.959
2024-07-23 14:33:02 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:33:02 Epoch 187/200 (lr=1e-05), train loss 0.00023, valid loss 0.00090
2024-07-23 14:33:25 Training for epoch 188 done, starting evaluation
2024-07-23 14:33:25 Epoch 188 performance:
2024-07-23 14:33:25 metrics/test.rmse:  2.172
2024-07-23 14:33:25 metrics/test.rmse_pcutoff:2.172
2024-07-23 14:33:25 metrics/test.mAP:   81.015
2024-07-23 14:33:25 metrics/test.mAR:   83.333
2024-07-23 14:33:25 metrics/test.mAP_pcutoff:81.015
2024-07-23 14:33:25 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:33:25 Epoch 188/200 (lr=1e-05), train loss 0.00017, valid loss 0.00095
2024-07-23 14:33:44 Training for epoch 189 done, starting evaluation
2024-07-23 14:33:44 Epoch 189 performance:
2024-07-23 14:33:44 metrics/test.rmse:  2.070
2024-07-23 14:33:44 metrics/test.rmse_pcutoff:2.070
2024-07-23 14:33:44 metrics/test.mAP:   83.886
2024-07-23 14:33:44 metrics/test.mAR:   85.000
2024-07-23 14:33:44 metrics/test.mAP_pcutoff:83.886
2024-07-23 14:33:44 metrics/test.mAR_pcutoff:85.000
2024-07-23 14:33:44 Epoch 189/200 (lr=1e-05), train loss 0.00014, valid loss 0.00082
2024-07-23 14:34:03 Training for epoch 190 done, starting evaluation
2024-07-23 14:34:04 Epoch 190 performance:
2024-07-23 14:34:04 metrics/test.rmse:  2.159
2024-07-23 14:34:04 metrics/test.rmse_pcutoff:2.159
2024-07-23 14:34:04 metrics/test.mAP:   78.911
2024-07-23 14:34:04 metrics/test.mAR:   81.667
2024-07-23 14:34:04 metrics/test.mAP_pcutoff:78.911
2024-07-23 14:34:04 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:34:04 Epoch 190/200 (lr=1e-06), train loss 0.00017, valid loss 0.00093
2024-07-23 14:34:23 Training for epoch 191 done, starting evaluation
2024-07-23 14:34:23 Epoch 191 performance:
2024-07-23 14:34:23 metrics/test.rmse:  2.143
2024-07-23 14:34:23 metrics/test.rmse_pcutoff:2.143
2024-07-23 14:34:23 metrics/test.mAP:   80.875
2024-07-23 14:34:23 metrics/test.mAR:   83.333
2024-07-23 14:34:23 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:34:23 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:34:23 Epoch 191/200 (lr=1e-06), train loss 0.00015, valid loss 0.00092
2024-07-23 14:34:43 Training for epoch 192 done, starting evaluation
2024-07-23 14:34:43 Epoch 192 performance:
2024-07-23 14:34:43 metrics/test.rmse:  2.142
2024-07-23 14:34:43 metrics/test.rmse_pcutoff:2.142
2024-07-23 14:34:43 metrics/test.mAP:   80.875
2024-07-23 14:34:43 metrics/test.mAR:   83.333
2024-07-23 14:34:43 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:34:43 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:34:43 Epoch 192/200 (lr=1e-06), train loss 0.00016, valid loss 0.00093
2024-07-23 14:35:02 Training for epoch 193 done, starting evaluation
2024-07-23 14:35:03 Epoch 193 performance:
2024-07-23 14:35:03 metrics/test.rmse:  2.144
2024-07-23 14:35:03 metrics/test.rmse_pcutoff:2.144
2024-07-23 14:35:03 metrics/test.mAP:   78.911
2024-07-23 14:35:03 metrics/test.mAR:   81.667
2024-07-23 14:35:03 metrics/test.mAP_pcutoff:78.911
2024-07-23 14:35:03 metrics/test.mAR_pcutoff:81.667
2024-07-23 14:35:03 Epoch 193/200 (lr=1e-06), train loss 0.00015, valid loss 0.00092
2024-07-23 14:35:22 Training for epoch 194 done, starting evaluation
2024-07-23 14:35:23 Epoch 194 performance:
2024-07-23 14:35:23 metrics/test.rmse:  2.125
2024-07-23 14:35:23 metrics/test.rmse_pcutoff:2.125
2024-07-23 14:35:23 metrics/test.mAP:   80.875
2024-07-23 14:35:23 metrics/test.mAR:   83.333
2024-07-23 14:35:23 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:35:23 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:35:23 Epoch 194/200 (lr=1e-06), train loss 0.00015, valid loss 0.00090
2024-07-23 14:35:41 Training for epoch 195 done, starting evaluation
2024-07-23 14:35:42 Epoch 195 performance:
2024-07-23 14:35:42 metrics/test.rmse:  2.104
2024-07-23 14:35:42 metrics/test.rmse_pcutoff:2.104
2024-07-23 14:35:42 metrics/test.mAP:   80.875
2024-07-23 14:35:42 metrics/test.mAR:   83.333
2024-07-23 14:35:42 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:35:42 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:35:42 Epoch 195/200 (lr=1e-06), train loss 0.00014, valid loss 0.00089
2024-07-23 14:36:01 Training for epoch 196 done, starting evaluation
2024-07-23 14:36:02 Epoch 196 performance:
2024-07-23 14:36:02 metrics/test.rmse:  2.110
2024-07-23 14:36:02 metrics/test.rmse_pcutoff:2.110
2024-07-23 14:36:02 metrics/test.mAP:   80.875
2024-07-23 14:36:02 metrics/test.mAR:   83.333
2024-07-23 14:36:02 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:36:02 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:36:02 Epoch 196/200 (lr=1e-06), train loss 0.00014, valid loss 0.00089
2024-07-23 14:36:21 Training for epoch 197 done, starting evaluation
2024-07-23 14:36:21 Epoch 197 performance:
2024-07-23 14:36:21 metrics/test.rmse:  2.108
2024-07-23 14:36:21 metrics/test.rmse_pcutoff:2.108
2024-07-23 14:36:21 metrics/test.mAP:   80.875
2024-07-23 14:36:21 metrics/test.mAR:   83.333
2024-07-23 14:36:21 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:36:21 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:36:21 Epoch 197/200 (lr=1e-06), train loss 0.00017, valid loss 0.00088
2024-07-23 14:36:41 Training for epoch 198 done, starting evaluation
2024-07-23 14:36:41 Epoch 198 performance:
2024-07-23 14:36:41 metrics/test.rmse:  2.117
2024-07-23 14:36:41 metrics/test.rmse_pcutoff:2.117
2024-07-23 14:36:41 metrics/test.mAP:   80.875
2024-07-23 14:36:41 metrics/test.mAR:   83.333
2024-07-23 14:36:41 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:36:41 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:36:41 Epoch 198/200 (lr=1e-06), train loss 0.00015, valid loss 0.00088
2024-07-23 14:37:01 Training for epoch 199 done, starting evaluation
2024-07-23 14:37:01 Epoch 199 performance:
2024-07-23 14:37:01 metrics/test.rmse:  2.128
2024-07-23 14:37:01 metrics/test.rmse_pcutoff:2.128
2024-07-23 14:37:01 metrics/test.mAP:   80.875
2024-07-23 14:37:01 metrics/test.mAR:   83.333
2024-07-23 14:37:01 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:37:01 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:37:01 Epoch 199/200 (lr=1e-06), train loss 0.00016, valid loss 0.00089
2024-07-23 14:37:21 Training for epoch 200 done, starting evaluation
2024-07-23 14:37:21 Epoch 200 performance:
2024-07-23 14:37:21 metrics/test.rmse:  2.106
2024-07-23 14:37:21 metrics/test.rmse_pcutoff:2.106
2024-07-23 14:37:21 metrics/test.mAP:   80.875
2024-07-23 14:37:21 metrics/test.mAR:   83.333
2024-07-23 14:37:21 metrics/test.mAP_pcutoff:80.875
2024-07-23 14:37:21 metrics/test.mAR_pcutoff:83.333
2024-07-23 14:37:22 Epoch 200/200 (lr=1e-06), train loss 0.00015, valid loss 0.00089
